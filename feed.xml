<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>轮子们</title>
    <link>http://blog.ywheel.com/feed/index.xml</link>
    <description>Recent content on 轮子们</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-CN</language>
    <copyright>Copyright (c) 2017. All rights reserved.</copyright>
    <lastBuildDate>Tue, 13 Feb 2018 10:08:23 +0800</lastBuildDate>
    <atom:link href="http://blog.ywheel.com/feed/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>多作业输出到同一组的多个目录的问题</title>
      <link>http://blog.ywheel.com/post/2018/02/13/multipleoutput</link>
      <pubDate>Tue, 13 Feb 2018 10:08:23 +0800</pubDate>
      
      <guid>http://blog.ywheel.com/post/2018/02/13/multipleoutput</guid>
      <description>

&lt;h2 id=&#34;场景1-一个作业输出到多个目录&#34;&gt;场景1：一个作业输出到多个目录&lt;/h2&gt;

&lt;h3 id=&#34;rddmultipletextoutputformat的实现&#34;&gt;RDDMultipleTextOutputFormat的实现&lt;/h3&gt;

&lt;p&gt;在一个典型的MR或者Spark作业中，作业输出到HDFS时会是一个目录，目录下将会根据分区写出成多个文件，比如&lt;code&gt;${outputDir}/part-r-00000&lt;/code&gt;。但是在一些场景下，我们希望能够在一个作业中，输出到多个目录中，变成多个数据集，后续的数据处理即可区分处理。&lt;/p&gt;

&lt;p&gt;这种场景的解决办法MultipleOutputFormat，很久之前就已经有了，在几年前写MapReduce程序的时候就已经实用过，网上一搜也一大堆。在这里，也只简单记录一下在Spark中如何使用。&lt;/p&gt;

&lt;p&gt;在Spark中，可以使用&lt;code&gt;saveToHadoopFile&lt;/code&gt;这个算子实现将RDD写入到HDFS，RDD的每个分区将会写出成HDFS上的一个文件，比如&lt;code&gt;part-00000&lt;/code&gt;. 这里相比MapReduce来说，命名上没有了中间的Task类型，及不区分m还是r。先看代码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  /**
   * Output the RDD to any Hadoop-supported file system, using a Hadoop `OutputFormat` class
   * supporting the key and value types K and V in this RDD.
   */
  def saveAsHadoopFile(
      path: String,
      keyClass: Class[_],
      valueClass: Class[_],
      outputFormatClass: Class[_ &amp;lt;: OutputFormat[_, _]],
      conf: JobConf = new JobConf(self.context.hadoopConfiguration),
      codec: Option[Class[_ &amp;lt;: CompressionCodec]] = None): Unit = self.withScope {
    // Rename this as hadoopConf internally to avoid shadowing (see SPARK-2038).
    val hadoopConf = conf
    hadoopConf.setOutputKeyClass(keyClass)
    hadoopConf.setOutputValueClass(valueClass)
    // Doesn&#39;t work in Scala 2.9 due to what may be a generics bug
    // TODO: Should we uncomment this for Scala 2.10?
    // conf.setOutputFormat(outputFormatClass)
    hadoopConf.set(&amp;quot;mapred.output.format.class&amp;quot;, outputFormatClass.getName)
    for (c &amp;lt;- codec) {
      hadoopConf.setCompressMapOutput(true)
      hadoopConf.set(&amp;quot;mapred.output.compress&amp;quot;, &amp;quot;true&amp;quot;)
      hadoopConf.setMapOutputCompressorClass(c)
      hadoopConf.set(&amp;quot;mapred.output.compression.codec&amp;quot;, c.getCanonicalName)
      hadoopConf.set(&amp;quot;mapred.output.compression.type&amp;quot;, CompressionType.BLOCK.toString)
    }

    // Use configured output committer if already set
    if (conf.getOutputCommitter == null) {
      hadoopConf.setOutputCommitter(classOf[FileOutputCommitter])
    }

    FileOutputFormat.setOutputPath(hadoopConf,
      SparkHadoopWriter.createPathFromString(path, hadoopConf))
    saveAsHadoopDataset(hadoopConf)
  }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;代码中，其中都是一些常规操作，在这里注意以下两点：
1. 参数中，可以指定OutputFormatClass， 这样我们就可以指定自定义的MultipleOutputFormat了，接下来会说明
2. 注意代码中，当没有显示指定OutputCommitter的时候，会默认使用FileOutputCommitter，一般场景下已经能满足。&lt;/p&gt;

&lt;p&gt;因此，需要先实现一个自定义的MultipleOutputFormat。在这里，我们假定spark处理时，产生了K/V. 希望能够按照K的内容，区分目录输出。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;class RDDMultipleTextOutputFormat[K, V]() extends MultipleTextOutputFormat[K, V]() {
  override def generateActualKey(key: K, value: V): K = {
    NullWritable.get().asInstanceOf[K]
  }

  override def generateFileNameForKeyValue(key: K, value: V, name: String): String = {
    key.toString
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在代码中，Key已经在之前的业务逻辑中，按照目录和文件名进行了赋值，因此，重写的generateFileNameForKeyValue只需要返回key的值即可。在generateFileNameForKeyValue的参数列表中,name可以认为是类似于&lt;code&gt;part-00000&lt;/code&gt;这样的字符串，在这里，因为key值中是包含了（子）路径和文件名的，比如&lt;code&gt;/key1/20180212/task-00000.csv&lt;/code&gt;,因此就不再需要name了，这个逻辑应该在前面的逻辑中保障。注意在这里也还需要重写generateActualKey返回NullWritable实例，在MultipleTextOutputFormat中，是使用TextOutputFormat输出，key为NullWritable实例时将不会写出key和K/V之间的分隔符，并且，从业务逻辑上来说，写出到HDFS的时候在当前的逻辑下是不需要写出Key到文件的，Key的内容已经在文件路径中了。&lt;/p&gt;

&lt;p&gt;MultipleOutputFormat的其他逻辑，可以细看代码，大体上就是根据文件名创建了多个RecoredWriter，保存在一个TreeMap中，每行记录输出时，将会找到对应的RecoredWriter进行输出。由于在MultipleTextOutputFormat中使用的是TextOutputFormat,因此，RecoredWriter事实上是LineRecoredWriter. 具体的在这里就不展开了。&lt;/p&gt;

&lt;p&gt;举一个简单的例子说明一下输出的结构。假设输出时，根目录为&lt;code&gt;/data&lt;/code&gt;, 根据业务需求，使用每行记录中的字段A的取值进行分目录输出，A的值域是省份代码，比如BJ, GD等。由于作业是每天运行一次，因此，也需要在省份目录下，按照日期创建子目录，真正的数据文件写在日期目录下，例如一个可能的文件路径为：&lt;code&gt;/data/GD/20180212/task-00000.csv&lt;/code&gt;, 这样，在上面的RDDMultipleTextOutputFormat.generateFileNameForKeyValue中，key的值为&lt;code&gt;/GD/20180212/task-00000.csv&lt;/code&gt;&lt;/p&gt;

&lt;h3 id=&#34;multipleoutput后的文件权限问题&#34;&gt;MultipleOutput后的文件权限问题&lt;/h3&gt;

&lt;p&gt;在上面的场景中，在输出到多个目录后，还隐藏着一个坑。我们知道，HDFS使用POSIX和ACL进行访问权限控制。对于ACL来说，default权限可以被子目录继承。&lt;/p&gt;

&lt;p&gt;我们当前的目录结构是&lt;code&gt;/data/${省份代码}/${日期}/${文件名}&lt;/code&gt;，数据是由集群ETL作业处理后写出的，也就是说&lt;code&gt;/data&lt;/code&gt;目录及子目录、文件的所有者是&lt;code&gt;etl&lt;/code&gt;账号, 并且, 集群默认的umask设置为了&lt;code&gt;007&lt;/code&gt;。在集群上分配业务账号使用时，需要对不同的账号授权，比如账号&lt;code&gt;guangdong&lt;/code&gt;只能访问&lt;code&gt;/data/GD/&lt;/code&gt;目录下的子目录和文件，具有只读权限，但对&lt;code&gt;/data/BJ/&lt;/code&gt;目录下的子目录和文件，是不可读的，相反，&lt;code&gt;beijing&lt;/code&gt;账号只能访问&lt;code&gt;/data/BJ/&lt;/code&gt;目录下的子目录和文件。&lt;/p&gt;

&lt;p&gt;因此，我们可以分别对两个目录进行ACL配置，使用如下命令进行授权：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;hadoop fs -setfacl -m -R default:user:guangdong:r-x /data/GD
hadoop fs -setfacl -m -R user:guangdong:r-x /data/GD
hadoop fs -setfacl -m -R default:user:beijing:r-x /data/BJ
hadoop fs -setfacl -m -R user:beijing:r-x /data/BJ
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这样，预期能实现区分账号访问数据的需求。但实际上却碰到了问题。&lt;/p&gt;

&lt;p&gt;由于已经执行了&lt;code&gt;setfacl&lt;/code&gt;操作，已有的目录、子目录和文件的权限正确，比如&lt;code&gt;/data/GD/20180210&lt;/code&gt;的ACL如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ hadoop fs -getfacl /data/GD/20180210
# file: /data/GD/20180210
# owner: etl
# group: etl
user::rwx
user:guangdong:r-x
group::rwx
mask::rwx
other::---
default:user::rwx
default:user:guangdong:r-x
default:group::rwx
default:mask::rwx
default:other::---
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;但是之后产生的子目录如&lt;code&gt;/data/GD/20180211&lt;/code&gt;，却没有继承ACL，导致访问时抛出如下错误：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Error: java.io.IOException: org.apache.hadoop.security.AccessControlException: Permission denied
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看ACL，也的确发现没有&lt;code&gt;guangdong&lt;/code&gt;这个用户在ACL里。&lt;/p&gt;

&lt;p&gt;后来发现，原来在文件路径&lt;code&gt;/data/GD/20180211/task-00000.csv&lt;/code&gt;生成的时候，&lt;code&gt;/data&lt;/code&gt;才是输出目录，而&lt;code&gt;/GD/20180211/task-00000.csv&lt;/code&gt;是MultipleOutputFormat输出的时候定义的文件名。因此，新产生的文件的ACL集成自&lt;code&gt;/data&lt;/code&gt;，而在&lt;code&gt;/data&lt;/code&gt;的ACL中，并没有配置&lt;code&gt;guangdong&lt;/code&gt;用户的default可读权限(在这里, &lt;code&gt;/data&lt;/code&gt;目录的POSIX设置为755)。&lt;/p&gt;

&lt;p&gt;但问题又来了，我们需要区分&lt;code&gt;/data/GD&lt;/code&gt;和&lt;code&gt;/data/BJ&lt;/code&gt;的授权，却又不得不在&lt;code&gt;/data&lt;/code&gt;目录配置&lt;code&gt;guangdong&lt;/code&gt;,&lt;code&gt;beijing&lt;/code&gt;用户的default可读权限，这两者相互矛盾。&lt;/p&gt;

&lt;p&gt;因此，我们只好按如下方式进行：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;对&lt;code&gt;/data&lt;/code&gt;目录添加业务账号（guangdong,beijing等）的default可读权限：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ hadoop fs -setfacl -m default:user:guangdong:r-x /data
$ hadoop fs -setfacl -m default:user:guangdong:r-x /data
$ hadoop fs -getfacl /data
# file: /data
# owner: etl
# group: etl
user::rwx
group::r-x
mask::rwx
other::r-x
default:user::rwx
default:user:guangdong:r-x
default:user:beijing:r-x
default:group::r-x
default:mask::rwx
default:other::r-x
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;默认创建&lt;code&gt;/data&lt;/code&gt;目录下的所有可能子目录，在这里，就是把所有省份代码都创建一遍，比如&lt;code&gt;/data/GD&lt;/code&gt;, &lt;code&gt;/data/BJ&lt;/code&gt;. 由于在&lt;code&gt;/data&lt;/code&gt;目录上配置了&lt;code&gt;guangdong&lt;/code&gt;,&lt;code&gt;beijing&lt;/code&gt;用户的default可读权限，因此ACL权限会被继承：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ hadoop fs -getfacl /data/GD
# file: /data/GD
# owner: etl
# group: etl
user::rwx
user:guangdong:r-x
user:beijing:r-x
group::rwx
mask::rwx
other::---
default:user::rwx
default:user:guangdong:r-x
default:user:beijing:r-x
default:group::rwx
default:mask::rwx
default:other::---
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;去除与各目录不相关的授权，比如&lt;code&gt;/data/GD&lt;/code&gt;, 应该只保留对&lt;code&gt;guangdong&lt;/code&gt;用户的可读权限，去掉&lt;code&gt;beijing&lt;/code&gt;用户的读权限。在这里default权限去掉不去掉都无所谓了，因为子目录实际仍是从&lt;code&gt;/data&lt;/code&gt;目录继承权限的。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ hadoop fs -setfacl -x user:beijing:r-x /data/GD
$ hadoop fs -setfacl -x user:guangdong:r-x /data/BJ
$ hadoop fs -getfacl /data/GD
# file: /data/GD
# owner: etl
# group: etl
user::rwx
user:guangdong:r-x
group::rwx
mask::rwx
other::---
default:user::rwx
default:user:guangdong:r-x
default:user:beijing:r-x
default:group::rwx
default:mask::rwx
default:other::---
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;这样，虽然ETL产生的新的文件仍然从&lt;code&gt;/data&lt;/code&gt;目录继承ACL权限，&lt;code&gt;beijing&lt;/code&gt;,&lt;code&gt;guangdong&lt;/code&gt;账号的可读权限都会被继承，但，在&lt;code&gt;/data&lt;/code&gt;目录下，省份代码这一层做了限制，也达到了区分账号控制文件访问权限的需求。&lt;/p&gt;

&lt;h2 id=&#34;场景2-多个作业输出到同一组目录&#34;&gt;场景2：多个作业输出到同一组目录&lt;/h2&gt;

&lt;p&gt;在上一个场景中，一个ETL作业，可使用MultipleOutputFormat根据数据内容，区分目录和文件输出，应该已经能够满足大部分的需求了。但，我们偏偏还碰到了另一个变态的需求：多个ETL作业输出到相同的根目录下。&lt;/p&gt;

&lt;p&gt;这个需求大概是这样产生的：所有的数据产生后都送到了Kafka中的同一个topic，并且由一个ETL作业来处理数据，并使用MultipleOutputFormat写出到HDFS。但由于数据量比较大，一个ETL作业难以处理（在这里原因有几个，不一一列举了），常出现处理时间超过预期，或者直接挂掉。那么，一个简单的做法，就是将数据送到不同的topic，再由多个不同的ETL作业进行处理。实际情况比这个更加复杂些，但我们在这里做一个简化描述，并假定按照中国北部、南部来区分，并由两个ETL作业来处理数据。这样，BJ的数据将由&lt;code&gt;ETL_North&lt;/code&gt;处理并写出，GD的数据将由&lt;code&gt;ELT_Sourth&lt;/code&gt;处理并写出.&lt;/p&gt;

&lt;p&gt;这样一看，似乎并没有任何问题，两个作业写出的目录并不冲突。但在作业跑起来后，却发现了问题：&lt;code&gt;_SUCCESS&lt;/code&gt;文件或&lt;code&gt;_temporary&lt;/code&gt;目录被删除导致作业失败。&lt;/p&gt;

&lt;p&gt;原来，我们使用的基于spark的ETL作业，写HDFS的时候使用了OutputCommitter的机制，其实与MapReduce一样，是为了保证作业各个task都成功才算是最终成功，因此，在作业运行过程中，会在输出目录下创建&lt;code&gt;_temporary&lt;/code&gt;目录存在task attempt的信息和数据，最终成功后再移动到最终的目录，并产生&lt;code&gt;_SUCCESS&lt;/code&gt;文件。但，由于我们使用了MultipleOutputFormat，其输出目录为&lt;code&gt;/data&lt;/code&gt;，因此，两个ETL作业，都在&lt;code&gt;/data&lt;/code&gt;目录下创建&lt;code&gt;_temporary&lt;/code&gt;并放置临时数据，当一个作业成功，另一个作业还未结束时，成功的ETL作业将会在将数据移动到最终目录后删除&lt;code&gt;_temporary&lt;/code&gt;目录，导致了另一个作业的失败。&lt;/p&gt;

&lt;p&gt;那么，如果我们能定制OutputCommitter，将&lt;code&gt;_SUCCESS&lt;/code&gt;文件和&lt;code&gt;_temporary&lt;/code&gt;目录重命名为不会冲突的名称，各作业之间不相互影响，则可解决问题。&lt;/p&gt;

&lt;p&gt;另外，由于我们使用的是&lt;code&gt;saveToHadoopFile&lt;/code&gt;算子，使用的是老的API，那么得使用一个wrapper封装一下。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;创建CustomerFileOutputCommitter(部分代码，其余代码与FileOutputCommitter一样)，将&lt;code&gt;PENDING_DIR_NAME&lt;/code&gt;和&lt;code&gt;SUCCEEDED_FILE_NAME&lt;/code&gt;变量改为不是final。注意这里继承的OutputCommitter是在&lt;code&gt;org.apache.hadoop.mapreduce&lt;/code&gt;包下:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public class SelfFileOutputCommitter extends OutputCommitter {
    private static final Log LOG = LogFactory.getLog(SelfFileOutputCommitter.class);

    /**
     * Name of directory where pending data is placed.  Data that has not been
     * committed yet.
     */
    public static String PENDING_DIR_NAME = &amp;quot;_temporary&amp;quot;;
    /**
     * Temporary directory name
     *
     * The static variable to be compatible with M/R 1.x
     */
    public static String SUCCEEDED_FILE_NAME = &amp;quot;_SUCCESS&amp;quot;;
    
    /* 此处省略大部分一样的代码 */
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;创建FileOutputCommitterWrapper，继承&lt;code&gt;org.apache.hadoop.mapred.OutputCommitter&lt;/code&gt;，封装CustomerFileOutputCommitter(部分)：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package com.ywheel.etl.lib.output;
    
import java.io.IOException;
    
import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;
import org.apache.hadoop.classification.InterfaceAudience;
import org.apache.hadoop.classification.InterfaceAudience.Private;
import org.apache.hadoop.classification.InterfaceStability;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.mapred.*;
    
@InterfaceAudience.Public
@InterfaceStability.Stable
public class FileOutputCommitterWrapper extends OutputCommitter {
    
    public static final Log LOG = LogFactory.getLog(&amp;quot;com.ywheel.etl.lib.output.FileOutputCommitterWrapper&amp;quot;);
    
    /**
     * Temporary directory name
     */
    public static String TEMP_DIR_NAME =
            CustomerFileOutputCommitter.PENDING_DIR_NAME;
    public static final String SUCCEEDED_FILE_NAME =
            CustomerFileOutputCommitter.SUCCEEDED_FILE_NAME;
    static final String SUCCESSFUL_JOB_OUTPUT_DIR_MARKER =
            CustomerFileOutputCommitter.SUCCESSFUL_JOB_OUTPUT_DIR_MARKER;
    
    private static Path getOutputPath(JobContext context) {
        JobConf conf = context.getJobConf();
        return FileOutputFormat.getOutputPath(conf);
    }
    
    private static Path getOutputPath(TaskAttemptContext context) {
        JobConf conf = context.getJobConf();
        return FileOutputFormat.getOutputPath(conf);
    }
    
    private CustomerFileOutputCommitter wrapped = null;
    
    private CustomerFileOutputCommitter
    getWrapped(JobContext context) throws IOException {
        if(wrapped == null) {
            wrapped = new CustomerFileOutputCommitter(
                    getOutputPath(context), context);
        }
        return wrapped;
    }
    
    private CustomerFileOutputCommitter
    getWrapped(TaskAttemptContext context) throws IOException {
        if(wrapped == null) {
            wrapped = new CustomerFileOutputCommitter(
                    getOutputPath(context), context);
        }
        return wrapped;
    }
    
    /* 此处省略大部分返回wrapped的方法 */
}
    
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;在&lt;code&gt;saveToHadoopFile&lt;/code&gt;之前，指定OutputCommitter（还记得上一章节看saveToHadoopFile算子代码的时候，里面有一行是设置默认的FileOutputCommitter吗）为FileOutputCommitterWrapper，并定义&lt;code&gt;_SUCCESS&lt;/code&gt;文件和&lt;code&gt;_temporary&lt;/code&gt;目录的值。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;val conf: JobConf = new JobConf(rdd.context.hadoopConfiguration)
CustomerFileOutputCommitter.PENDING_DIR_NAME = etlJobName + &amp;quot;_temporary&amp;quot;
CustomerFileOutputCommitter.SUCCEEDED_FILE_NAME = etlJobName + &amp;quot;_SUCCESS&amp;quot;
conf.setOutputCommitter(classOf[FileOutputCommitterWrapper])
// next: rdd.saveToHadoopFile(outPutPath + File.separator, classOf[String], classOf[String], classOf[RDDMultipleTextOutputFormat[String, String]],conf)
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;这样改造后，各作业运行就相互不干扰了。在实际场景中，还可能出现多个作业往同一个叶子目录(本文讲的是中间路径而不是叶子目录)写文件的情况，那这个时候，除了考虑上面的Committer相关问题外，还需要考虑最后的文件名也不能冲突。比如每个作业输出在叶子目录中的文件名就不能是&lt;code&gt;task-00000.csv&lt;/code&gt;了，而应该也要加上与作业相关的信息，比如&lt;code&gt;etl_north_task_00000.csv&lt;/code&gt;,&lt;code&gt;etl_sourth_task_00000.csv&lt;/code&gt;, 这样即便最后的文件在一个目录下也不再冲突。&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;想要对hadoop核心代码做一点contribution难，不过，在本文章内相关代码的时候，发现了一处拼写错误，遂改之: &lt;a href=&#34;https://issues.apache.org/jira/browse/MAPREDUCE-7051&#34;&gt;https://issues.apache.org/jira/browse/MAPREDUCE-7051&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Linux服务器监控的神器：Netdata</title>
      <link>http://blog.ywheel.com/post/2017/03/26/netdata</link>
      <pubDate>Sun, 26 Mar 2017 23:15:46 +0800</pubDate>
      
      <guid>http://blog.ywheel.com/post/2017/03/26/netdata</guid>
      <description>

&lt;p&gt;由于工作的关系，最近在思考如何做集群、服务器的监控。在网上东转转西转转，偶然发现了一个单机监控的2016新秀Netdata，眼前着实为之一亮。 令人印象非常之深刻的个主要特性：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;界面酷炫，实时监控&lt;/li&gt;
&lt;li&gt;零配置，即装即用&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;官网地址在这里： &lt;a href=&#34;https://my-netdata.io/&#34;&gt;https://my-netdata.io/&lt;/a&gt; ， 在&lt;a href=&#34;https://octoverse.github.com/&#34;&gt;The state of the Octoverse 2016&lt;/a&gt; 也能看到他的身影：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://o75oehjrs.bkt.clouddn.com/image/blog/GitHub%20octoverse%202016.png?watermark/2/text/YmxvZy55d2hlZWwuY24=/font/5b6u6L2v6ZuF6buR/fontsize/500/fill/I0Y1RUZFRg==/dissolve/100/gravity/SouthEast/dx/10/dy/10&#34; alt=&#34;github octoverse 2016&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;netdata-feature&#34;&gt;Netdata feature&lt;/h2&gt;

&lt;p&gt;从&lt;a href=&#34;https://github.com/firehol/netdata#features&#34;&gt;Github&lt;/a&gt;上能够看到netdata的主要功能，主要有几点（详细的可查看github上的说明）：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;interactive bootstrap dashboards, 酷炫（主要是dark主题，light主题就没这感觉了）&lt;/li&gt;
&lt;li&gt;匪夷所思的快。。。所有请求每个metreic都在0.5ms内响应，即便是一台烂机器&lt;/li&gt;
&lt;li&gt;非常高效，每秒采集数千个指标，但仅占cpu单核1%，少量MB的内存以及完全没有磁盘IO&lt;/li&gt;
&lt;li&gt;提供复杂的、各种类型的告警，支持动态阈值、告警模板、多种通知方式等&lt;/li&gt;
&lt;li&gt;可扩展，使用自带的插件API（比如bash, python, perl, node.js, java, go, ruby等）来收集任何可以衡量的数据&lt;/li&gt;
&lt;li&gt;零配置：安装后netdata会自动的监测一切&lt;/li&gt;
&lt;li&gt;零依赖：netdata有自己的web server， 提供静态web文件和web API&lt;/li&gt;
&lt;li&gt;零维护：只管跑上！&lt;/li&gt;
&lt;li&gt;支撑多种时间序列后端服务，比如graphite, opentsdb, prometheus, json document DBs&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Netdata监控项也很多，比如CPU, 内存，磁盘，网络这些基础的之外，还可以有IPC, netfilter/iptables Linux firewall, fping, Processes, NFS, &lt;code&gt;Network QoS&lt;/code&gt;, Applications, Apache web server, Nginx, Tomcat, mysql, postgres, redis, mongodb, elasticsearch, SNMP devices等等。&lt;/p&gt;

&lt;h2 id=&#34;netdata-install&#34;&gt;Netdata install&lt;/h2&gt;

&lt;p&gt;Netdata的安装非常简单，支持几乎所有的Linux版本。刚好我还有一个用于来科学上网的EC2机器是Unbutu系统，果断登上去尝试。&lt;/p&gt;

&lt;h3 id=&#34;安装准备&#34;&gt;安装准备&lt;/h3&gt;

&lt;p&gt;Netdata提供了一个非常简便的安装方法，我的Unbutu系统只需要执行下面的命令即可完成安装netdata所依赖的各种东西：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;curl -Ss &#39;https://raw.githubusercontent.com/firehol/netdata-demo-site/master/install-required-packages.sh&#39; &amp;gt;/tmp/kickstart.sh &amp;amp;&amp;amp; bash /tmp/kickstart.sh -i netdata
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;注意，上面的命令是安装基本的部分，不包括&lt;code&gt;mysql / mariadb, postgres, named, hardware sensors and SNMP&lt;/code&gt;. 如果要完整安装，则需要执行下面的命令：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;curl -Ss &#39;https://raw.githubusercontent.com/firehol/netdata-demo-site/master/install-required-packages.sh&#39; &amp;gt;/tmp/kickstart.sh &amp;amp;&amp;amp; bash /tmp/kickstart.sh -i netdata-all
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;安装netdata&#34;&gt;安装Netdata&lt;/h3&gt;

&lt;p&gt;安装Netdata也很简单，按照wiki的说明即可：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# download it - the directory &#39;netdata&#39; will be created
git clone https://github.com/firehol/netdata.git --depth=1
cd netdata

# run script with root privileges to build, install, start netdata
./netdata-installer.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;注意上面要使用root权限，执行命令后的提示信息也很丰富有趣，比如刚开头是这样的：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo ./netdata-installer.sh 

  ^
  |.-.   .-.   .-.   .-.   .  netdata                                        
  |   &#39;-&#39;   &#39;-&#39;   &#39;-&#39;   &#39;-&#39;   real-time performance monitoring, done right!  
  +----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+---&amp;gt;


  You are about to build and install netdata to your system.

  It will be installed at these locations:

   - the daemon     at /usr/sbin/netdata
   - config files   in /etc/netdata
   - web files      in /usr/share/netdata
   - plugins        in /usr/libexec/netdata
   - cache files    in /var/cache/netdata
   - db files       in /var/lib/netdata
   - log files      in /var/log/netdata
   - pid file       at /var/run/netdata.pid
   - logrotate file at /etc/logrotate.d/netdata

  This installer allows you to change the installation path.
  Press Control-C and run the same command with --help for help.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;安装结束的最后几行是这样的：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Uninstall script generated: ./netdata-uninstaller.sh
Update script generated   : ./netdata-updater.sh

netdata-updater.sh can work from cron. It will trigger an email from cron
only if it fails (it does not print anything if it can update netdata).
Run this to automatically check and install netdata updates once per day:

ln -s /home/ubuntu/netdata/netdata-updater.sh /etc/cron.daily/netdata-updater.sh

 --- We are done! --- 

  ^
  |.-.   .-.   .-.   .-.   .-.   .  netdata                          .-.   .-
  |   &#39;-&#39;   &#39;-&#39;   &#39;-&#39;   &#39;-&#39;   &#39;-&#39;   is installed and running now!  -&#39;   &#39;-&#39;  
  +----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+---&amp;gt;

  enjoy real-time performance and health monitoring...

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;安装完后，还可以根据wiki所说的配置开机启动，照做之后执行&lt;code&gt;service netdata start&lt;/code&gt;启动服务，可以访问&lt;a href=&#34;http://localhost:19999/&#34;&gt;http://localhost:19999/&lt;/a&gt; 看到监控界面。随后去AWS控制台放通19999端口，我的EC2机器的监控系统就大功告成啦！&lt;/p&gt;

&lt;p&gt;由此看出，Netdata的安装非常之简单，只有几行命令，而且根本无需配置。&lt;/p&gt;

&lt;h3 id=&#34;监控页面&#34;&gt;监控页面&lt;/h3&gt;

&lt;p&gt;再来看看监控页面，除了配色酷炫，监控项种类繁多之外，页面元素的实时响应、告警设置等都极具亮点。为了更好的展示页面，在这里会盗用github上netdata官方的几个动态图来show一下.&lt;/p&gt;

&lt;h4 id=&#34;system-overview&#34;&gt;System overview&lt;/h4&gt;

&lt;p&gt;&lt;img src=&#34;https://cloud.githubusercontent.com/assets/2662304/14092712/93b039ea-f551-11e5-822c-beadbf2b2a2e.gif&#34; alt=&#34;System overview&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;disks&#34;&gt;Disks&lt;/h4&gt;

&lt;p&gt;&lt;img src=&#34;https://cloud.githubusercontent.com/assets/2662304/14093195/c882bbf4-f554-11e5-8863-1788d643d2c0.gif&#34; alt=&#34;Disks&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;network-interfaces&#34;&gt;Network interfaces&lt;/h4&gt;

&lt;p&gt;&lt;img src=&#34;https://cloud.githubusercontent.com/assets/2662304/14093128/4d566494-f554-11e5-8ee4-5392e0ac51f0.gif&#34; alt=&#34;Network&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;alarms&#34;&gt;Alarms&lt;/h4&gt;

&lt;p&gt;&lt;img src=&#34;http://o75oehjrs.bkt.clouddn.com/image/blog/netdata%20alarm.png?watermark/2/text/YmxvZy55d2hlZWwuY24=/font/5b6u6L2v6ZuF6buR/fontsize/500/fill/I0Y1RUZFRg==/dissolve/100/gravity/SouthEast/dx/10/dy/10&#34; alt=&#34;Alarms&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;netdata-backend&#34;&gt;Netdata backend&lt;/h2&gt;

&lt;p&gt;Netdata也可以后台服务收集监控指标，多服务器的监控指标汇总到前台展示，或者归档汇总后提供给其他工具如grafana， 如下图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://cloud.githubusercontent.com/assets/2662304/20649711/29f182ba-b4ce-11e6-97c8-ab2c0ab59833.png&#34; alt=&#34;netdata backend&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Netdata支持如下几个backends:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;1) graphite;&lt;/li&gt;
&lt;li&gt;2) opentsdb;&lt;/li&gt;
&lt;li&gt;3) json document DBs.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;并能够提供3种计算模式：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;1) as collected；&lt;/li&gt;
&lt;li&gt;2）average;&lt;/li&gt;
&lt;li&gt;3) sum or volume。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;具体的可以到&lt;a href=&#34;https://github.com/firehol/netdata/wiki/netdata-backends&#34;&gt;netdata wiki&lt;/a&gt;查看。利用这种方式，应该也较容易能够折腾出来一个集群监控的解决方案，并且netdata和grafana的界面看起来都非常的酷炫（又一次印证了一个观点：大屏监控系统就得是暗色系！）&lt;/p&gt;

&lt;p&gt;看到&lt;a href=&#34;https://github.com/firehol/netdata/wiki#is-there-a-roadmap&#34;&gt;roadmap&lt;/a&gt;里面提到：monitor more applications (hadoop and friends, postgres, etc). 也希望hadoop这方面的监控能早日实现，又可以多一个可选方案啦~&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Mac上搭建ELK</title>
      <link>http://blog.ywheel.com/post/2017/03/04/setup_elk_on_mac/</link>
      <pubDate>Sat, 04 Mar 2017 20:52:39 +0800</pubDate>
      
      <guid>http://blog.ywheel.com/post/2017/03/04/setup_elk_on_mac/</guid>
      <description>

&lt;p&gt;最近的项目需要对文本数据各字段进行快速检索、组合查询、模糊查询，在架构选择上选择了Elasticsearch作为支撑这些功能的存储和搜索引擎。其他的不说了，刚好我的第一台mac到了，直接搞起。&lt;/p&gt;

&lt;h2 id=&#34;什么是elk&#34;&gt;什么是ELK&lt;/h2&gt;

&lt;p&gt;日志分析平台可以有多种技术架构的选型，但经过了多年的演变，现在比较流行的应该就是ELK了。 ELK三个字母的意义如下：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Elasticsearch&lt;/li&gt;
&lt;li&gt;Logstash&lt;/li&gt;
&lt;li&gt;Kibana&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;架构图下图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://o75oehjrs.bkt.clouddn.com/image/blog/ELK%E6%9E%B6%E6%9E%84.jpg?imageMogr2/auto-orient/thumbnail/800x/blur/1x0/quality/75|watermark/2/text/YmxvZy55d2hlZWwuY24=/font/5a6L5L2T/fontsize/560/fill/I0ZBRkFGQQ==/dissolve/100/gravity/SouthWest/dx/10/dy/10|imageslim&#34; alt=&#34;ELK架构图&#34; /&gt;&lt;/p&gt;

&lt;p&gt;图中的Shipper和Indexer都可以是Logstash, Broker一般为Redis，也可以是kafka等。而Search &amp;amp; Storage则主要是Elasticsearch了，一方面接收上游index好的文档，另一方面提供API支持对内容的检索。而kibana则是一个web interface, 可以提供简单易用的界面让用户方便的写出搜索的表达式来访问Elasticsearch.&lt;/p&gt;

&lt;p&gt;对于这三部分都有很多深入的点，以后有机会要深入学习和记录。&lt;/p&gt;

&lt;h2 id=&#34;使用brew安装&#34;&gt;使用brew安装&lt;/h2&gt;

&lt;p&gt;既然使用了mac，那么使用brew安装程序则是一个最简单不过的方式了。&lt;/p&gt;

&lt;p&gt;首先安装Elasticsearch，直接输入以下命令即可:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;brew install elasticsearch
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;但可能遇到问题，比如要求Java的版本是1.8（我安装的Elasticsearch的按本是5.2.2），这里面可能涉及到还要安装&lt;code&gt;brew cast&lt;/code&gt;用来安装java8, 然后又提示还有其他依赖（后悔没记录下来。。。），而需要xcode-command-tool, 折腾了不少时间。&lt;/p&gt;

&lt;p&gt;安装完成后，可以查看elasticsearch的版本：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ elasticsearch --version
Version: 5.2.2, Build: f9d9b74/2017-02-24T17:26:45.835Z, JVM: 1.8.0_121
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;启动和停止elasticsearch也很简单：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;brew services start elasticsearch
brew services stop elasticsearch
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;浏览器访问&lt;code&gt;http://localhost:9200&lt;/code&gt;可以看到Elasticsearch的信息：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{
name: &amp;quot;bWXgrRX&amp;quot;,
cluster_name: &amp;quot;elasticsearch_ywheel&amp;quot;,
cluster_uuid: &amp;quot;m99a1gFWQzKECuwnBfnTug&amp;quot;,
version: {
number: &amp;quot;5.2.2&amp;quot;,
build_hash: &amp;quot;f9d9b74&amp;quot;,
build_date: &amp;quot;2017-02-24T17:26:45.835Z&amp;quot;,
build_snapshot: false,
lucene_version: &amp;quot;6.4.1&amp;quot;
},
tagline: &amp;quot;You Know, for Search&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;接着安装logstash：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;brew install logstash
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;安装好后查看版本：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ logstash --version
logstash 5.2.2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;浏览器访问&lt;code&gt;http://localhost:9600&lt;/code&gt;可以看到如下信息：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{
host: &amp;quot;ywheeldeMacBook-Pro.local&amp;quot;,
version: &amp;quot;5.2.2&amp;quot;,
http_address: &amp;quot;127.0.0.1:9600&amp;quot;,
id: &amp;quot;70b78f4a-fe0f-4187-bf71-fe1f60b74e0a&amp;quot;,
name: &amp;quot;ywheeldeMacBook-Pro.local&amp;quot;,
build_date: &amp;quot;2017-02-24T17:46:55Z&amp;quot;,
build_sha: &amp;quot;57984d20eb28b0df40a59077c600ec1a399d46f5&amp;quot;,
build_snapshot: false
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Kibana不需要通过brew安装，直接下载压缩包后，解压后执行&lt;code&gt;./kibana&lt;/code&gt;即可。不过我还是在&lt;code&gt;/usr/local/bin/&lt;/code&gt;下创建了&lt;code&gt;kibana&lt;/code&gt;和&lt;code&gt;kibana-plugin&lt;/code&gt;的软连接， &lt;code&gt;elasticsearch&lt;/code&gt;,&lt;code&gt;elasticsearch-plugin&lt;/code&gt;,&lt;code&gt;logstash&lt;/code&gt;和&lt;code&gt;logstash-plugin&lt;/code&gt;都在这个目录下，以后安装插件的话，还都需要用上这些&lt;code&gt;*-plugin&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Kibana安装完成后，需要在&lt;code&gt;config/kibana.yml&lt;/code&gt;文件中，确认&lt;code&gt;elasticsearch.url: &amp;quot;http://localhost:9200&amp;quot;&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&#34;测试写入和查询&#34;&gt;测试写入和查询&lt;/h2&gt;

&lt;p&gt;写入Elasticsearch可以很简单，其本身就提供了RESTFul的API接口，参考&lt;a href=&#34;https://www.elastic.co/guide/en/kibana/3.0/import-some-data.html&#34;&gt;https://www.elastic.co/guide/en/kibana/3.0/import-some-data.html&lt;/a&gt; ，通过以下命令创建shakespeare index：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;curl -XPUT http://localhost:9200/shakespeare -d &#39;
{
 &amp;quot;mappings&amp;quot; : {
  &amp;quot;_default_&amp;quot; : {
   &amp;quot;properties&amp;quot; : {
    &amp;quot;speaker&amp;quot; : {&amp;quot;type&amp;quot;: &amp;quot;string&amp;quot;, &amp;quot;index&amp;quot; : &amp;quot;not_analyzed&amp;quot; },
    &amp;quot;play_name&amp;quot; : {&amp;quot;type&amp;quot;: &amp;quot;string&amp;quot;, &amp;quot;index&amp;quot; : &amp;quot;not_analyzed&amp;quot; },
    &amp;quot;line_id&amp;quot; : { &amp;quot;type&amp;quot; : &amp;quot;integer&amp;quot; },
    &amp;quot;speech_number&amp;quot; : { &amp;quot;type&amp;quot; : &amp;quot;integer&amp;quot; }
   }
  }
 }
}
&#39;;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通过以下命令将数据写入Elasticsearch：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;curl -XPUT localhost:9200/_bulk --data-binary @shakespeare.json
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;数据写入后，到kibana目录运行&lt;code&gt;./kibana&lt;/code&gt;，启动后访问：&lt;code&gt;http://localhost:5601/&lt;/code&gt; , 看到kibana界面后会提示&amp;rdquo;Configure an index pattern&amp;rdquo;。， 如下图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://o75oehjrs.bkt.clouddn.com/image/blog/kibana%E5%88%9B%E5%BB%BAindex%201.png?watermark/2/text/YmxvZy55d2hlZWwuY24=/font/5a6L5L2T/fontsize/500/fill/Izk3QjhGMw==/dissolve/100/gravity/SouthEast/dx/10/dy/10&#34; alt=&#34;kibana 1&#34; /&gt;&lt;/p&gt;

&lt;p&gt;刚才在写入数据的时候已经创建了shakespeare index, 且不是按照时间分布的日志文件（shakespeare只有一个json文件），因此，取消勾选&lt;code&gt;Index contains time-based envents&lt;/code&gt;，输入&lt;code&gt;shakespeare&lt;/code&gt;后，就能看到&lt;code&gt;create&lt;/code&gt;按钮了。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://o75oehjrs.bkt.clouddn.com/image/blog/kibana%E5%88%9B%E5%BB%BAindex%202.png?watermark/2/text/YmxvZy55d2hlZWwuY24=/font/5a6L5L2T/fontsize/500/fill/Izk3QjhGMw==/dissolve/100/gravity/SouthEast/dx/10/dy/10&#34; alt=&#34;kibana 2&#34; /&gt;&lt;/p&gt;

&lt;p&gt;点击kibana的Discover页面，输入&lt;code&gt;WESTMORELAND&lt;/code&gt;查询，可以看到有110个结果：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://o75oehjrs.bkt.clouddn.com/image/blog/kibana%20discover.png?watermark/2/text/YmxvZy55d2hlZWwuY24=/font/5a6L5L2T/fontsize/500/fill/Izk3QjhGMw==/dissolve/100/gravity/SouthEast/dx/10/dy/10&#34; alt=&#34;kibana 3&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;监控和安全&#34;&gt;监控和安全&lt;/h2&gt;

&lt;p&gt;在Elasticsearch 5.x的时代，监控和管理由X-Pack统一完成，包含：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;安全：用户权限管理&lt;/li&gt;
&lt;li&gt;告警：自动告警&lt;/li&gt;
&lt;li&gt;监控：监控Elasticsearch集群的状态&lt;/li&gt;
&lt;li&gt;报告：发送报告、导出数据&lt;/li&gt;
&lt;li&gt;图表：可视化数据&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;在安装X-Pack之前，需要停止Kibana和Elasticsearch:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;elasticsearch-plugin install x-pack
kibana-plugin install x-pack
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;安装完成后，启动elasticsearch和kibana，访问kibana时发现需要登录了， 默认用户名和密码是elastic/changeme。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://o75oehjrs.bkt.clouddn.com/image/blog/kibana%20login.png?watermark/2/text/YmxvZy55d2hlZWwuY24=/font/5a6L5L2T/fontsize/500/fill/Izk3QjhGMw==/dissolve/100/gravity/SouthEast/dx/10/dy/10&#34; alt=&#34;kibana login&#34; /&gt;&lt;/p&gt;

&lt;p&gt;后续可以在Management面板中进行用户和角色的配置，也可以看到新增了Reporting。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://o75oehjrs.bkt.clouddn.com/image/blog/kibana%20management.png?watermark/2/text/YmxvZy55d2hlZWwuY24=/font/5a6L5L2T/fontsize/500/fill/Izk3QjhGMw==/dissolve/100/gravity/SouthEast/dx/10/dy/10&#34; alt=&#34;kibana management&#34; /&gt;&lt;/p&gt;

&lt;p&gt;在Monitoring页面中可以看到Elasticsearch和Kibana的状态，点击Indices还可以看到具体索引的状态。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://o75oehjrs.bkt.clouddn.com/image/blog/kibana%20monitoring%201.png?watermark/2/text/YmxvZy55d2hlZWwuY24=/font/5a6L5L2T/fontsize/500/fill/Izk3QjhGMw==/dissolve/100/gravity/SouthEast/dx/10/dy/10&#34; alt=&#34;kibana monitoring 1&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://o75oehjrs.bkt.clouddn.com/image/blog/kibana%20monitoring%202.png?watermark/2/text/YmxvZy55d2hlZWwuY24=/font/5a6L5L2T/fontsize/500/fill/Izk3QjhGMw==/dissolve/100/gravity/SouthEast/dx/10/dy/10&#34; alt=&#34;kibana monitoring 1&#34; /&gt;&lt;/p&gt;

&lt;p&gt;告警功能和报表功能后续再进行详细研究。之前在A家的时候，记得有个基于日志的告警功能：当service的日志中出现了ERROR或FATAL，可以自动触发告警。有了X-Pack后，这个功能应该也是可以通过ELK来实现的啦。&lt;/p&gt;

&lt;p&gt;通过访问&lt;code&gt;http://localhost:9200/_cat/indices?v&lt;/code&gt;查看Elasticsearch的Index, 可以发现几个新的与监控有关的index:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;health status index                           uuid                   pri rep docs.count docs.deleted store.size pri.store.size
yellow open   .monitoring-es-2-2017.03.04     COZvO_dlSkqdEtntrZrzFA   1   1      10240          154      4.5mb          4.5mb
green  open   .security                       XEeHRF5NT0ud2jpxOzsoHw   1   0          1            0      2.8kb          2.8kb
yellow open   .kibana                         p-cJGBCXQySNGR0924jRdQ   1   1          2            1      9.8kb          9.8kb
yellow open   .monitoring-data-2              QZt0hpTISUO_58pWoG5Hyw   1   1          3            0      6.9kb          6.9kb
yellow open   .monitoring-kibana-2-2017.03.04 nLHKuL1KTiCE2lsWz8tdkA   1   1        849            0      245kb          245kb
yellow open   shakespeare                     zPCLp4KmTkiu7m4tYcA_Iw   5   1     111396            0     28.1mb         28.1mb
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;使用logstash导入博文&#34;&gt;使用Logstash导入博文&lt;/h2&gt;

&lt;p&gt;在上面的操作中，直接使用了elasticsearch的API接口来进行数据的导入，而使用logstash也能够很方便的写入elasticsearch。 首先得生成一个logstash的conf文件，比如我想建立我的博客的索引，在我的家目录下创建了my_blog.conf文件：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;input{
    file{
        path =&amp;gt; [&amp;quot;/Users/ywheel/my_blog/content/about.md&amp;quot;]
    }
}   
output{
    elasticsearch{
        hosts =&amp;gt; [&amp;quot;localhost:9200&amp;quot;]
        index =&amp;gt; &amp;quot;my_blog&amp;quot;
        user =&amp;gt; &amp;quot;elastic&amp;quot;
        password =&amp;gt; &amp;quot;changeme&amp;quot;
   }
}

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;注意拜X-Pack所赐，这配置文件里面对elasticsearch需要用户名和密码。然后敲入&lt;code&gt;logstash -f my_blog.conf&lt;/code&gt;来执行，但却一直不成功。后来翻logstash的文档 &lt;a href=&#34;https://www.elastic.co/guide/en/logstash/current/pipeline.html&#34;&gt;https://www.elastic.co/guide/en/logstash/current/pipeline.html&lt;/a&gt; ，里面写了一句这样的话：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Inputs

You use inputs to get data into Logstash. Some of the more commonly-used inputs are:

- file: reads from a file on the filesystem, much like the UNIX command tail -0F
- syslog: listens on the well-known port 514 for syslog messages and parses according to the RFC3164 format
- redis: reads from a redis server, using both redis channels and redis lists. Redis is often used as a &amp;quot;broker&amp;quot; in a centralized Logstash installation, which queues Logstash events from remote Logstash &amp;quot;shippers&amp;quot;.
- beats: processes events sent by Filebeat.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;file&lt;/code&gt;这个input相当于使用&lt;code&gt;tail&lt;/code&gt;来获取文件中的数据的啊， 我的about.md压根就没有变化，因此也没有内容被写入了Elasticsearch。于是，我把conf改成了这样：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;input{
    file{
        path =&amp;gt; [&amp;quot;/Users/ywheel/test.md&amp;quot;]
    }
}   
output{
    elasticsearch{
        hosts =&amp;gt; [&amp;quot;localhost:9200&amp;quot;]
        index =&amp;gt; &amp;quot;my_blog&amp;quot;
        user =&amp;gt; &amp;quot;elastic&amp;quot;
        password =&amp;gt; &amp;quot;changeme&amp;quot;
   }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;运行&lt;code&gt;logstash -f my_blog_conf&lt;/code&gt;后，再运行&lt;code&gt;cat /Users/ywheel/my_blog/content/about.md &amp;gt; /Users/ywheel/test.md&lt;/code&gt;, 然后发现数据写入了Elasticsearch， index也多了一个&lt;code&gt;my_blog&lt;/code&gt;。到Kibana中添加&lt;code&gt;my_blog&lt;/code&gt;这个index pattern后，就可以在Discover进行搜索了。比如我搜索“程序员”：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://o75oehjrs.bkt.clouddn.com/image/blog/kibana%20search.png?watermark/2/text/YmxvZy55d2hlZWwuY24=/font/5a6L5L2T/fontsize/500/fill/Izk3QjhGMw==/dissolve/100/gravity/SouthEast/dx/10/dy/10&#34; alt=&#34;kibana search&#34; /&gt;&lt;/p&gt;

&lt;p&gt;看来中文分词得改进一下，不过现在也已经很酷了! 以后可以对整个博客进行全文检索了~~&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>我的第一台Mac</title>
      <link>http://blog.ywheel.com/post/2017/03/04/my_first_macbookpro/</link>
      <pubDate>Sat, 04 Mar 2017 19:43:18 +0800</pubDate>
      
      <guid>http://blog.ywheel.com/post/2017/03/04/my_first_macbookpro/</guid>
      <description>&lt;p&gt;我的上一篇文章，还是去年7月份的了，中间隔了大半年。大半年来的忙碌工作，让这个博客就像一个烂尾楼，半死不活，仅靠着几篇文章苟活着。当然，懒也是逃不掉的一个原因。回想这半年多来的工作，在工作内容和职责上出现了一些转变，使得我自己写代码的时间变得越来越少。还记得最近一次写代码还是自己给自己分配的一个单元测试的代码样例，但也同样超出了时间预期，自己成为了整个团队中拖后腿的人。&lt;/p&gt;

&lt;p&gt;工作内容和职责的变化，也导致我需要&amp;rdquo;频繁&amp;rdquo;地出差，2016年共飞了25次，对于一个技术人员来说这个飞行次数显然是一个过大的值。即便在广州也需要频繁地到现场面对客户，写PPT，讲PPT。&lt;/p&gt;

&lt;p&gt;于是我时而为不能写代码而焦虑。从去年年中开始装修房子时，我尽力争取保留下了最小的卧室作为我的书房，就开始构想我要买一张什么样的桌子，换一台电脑，在家也布置一个适宜写代码的环境。慢慢的， 我买回了桌椅、DELL的显示器、Cherry MX 3.0红轴、Cherry的超大鼠标垫。最后，终于等来了我的第一台Macbook pro.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://o75oehjrs.bkt.clouddn.com/image/blog/shufang2.jpg?imageMogr2/auto-orient/thumbnail/800x/blur/1x0/quality/75|watermark/2/text/YmxvZy55d2hlZWwuY24=/font/5a6L5L2T/fontsize/560/fill/I0ZBRkFGQQ==/dissolve/100/gravity/SouthWest/dx/10/dy/10|imageslim&#34; alt=&#34;书房图&#34; /&gt;&lt;/p&gt;

&lt;p&gt;严格的说, 这不是我使用的第一台MBP。两年前我还在北京的时候，前东家对SDE这个职位是默认同意采购macbook pro作为工作使用电脑的，或者是员工私人的macbook拿到IT那里去安装定制的系统，当然有一个限制：不能是macbook air，理由居然是air的性能跟不上工作的需要。好吧，反正我使用着公司的mbp，用了半年多后离开了，mbp也留给了同组的同事，现在可能也把它带去了美国。&lt;/p&gt;

&lt;p&gt;在来到广州后，公司居然对研发人员只配备了一台E系列的联想笔记本，看着也不想称呼他为thinkpad。在无法忍受每天只能对着笔记本的小屏幕开发程序后，我立刻买了一个显示器和键鼠套装，是程序员就该对自己的开发环境负责。&lt;/p&gt;

&lt;p&gt;虽说我的主要开发语言是java，开发环境在windows、Linux、Mac OS下其实都没有太大所谓，但编译打包、代码提交等还是遇到了挺多头大的问题。比如：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;git命令行不友好。作为深度git用户，虽然有git的客户端如github destkop/sourcetree等，我还是觉得git的命令敲起来爽（嘿嘿也许是cherry的手感问题？）. 然后windows下的git bash经常无故崩溃，尤其有段时间一使用git pull，电脑直接蓝屏，只能按电源键强制关机重启来解决。&lt;/li&gt;
&lt;li&gt;匪夷所思的权限。在使用git切换分支的时候，报无写权限导致切换分支失败；或者是使用git stash pop将之前缓存的修改拿出来的时候，无写权限导致stash pop失败，搞了半天也不知如何是好，只好放弃。这就好比敲了这么多字没保存电脑死机一样的痛苦，于是只好静下心来回忆缓存的代码实现的功能和方式，顺便重构和优化一把。&lt;/li&gt;
&lt;li&gt;没有终端，没有终端，没有终端，重要的事情说三遍！&lt;/li&gt;
&lt;li&gt;不够酷&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;在等了又等之后，苹果终于发布了2016款的macbook，看完后略伤心，脑补了一下携带各种转换头的场景后，我还是选择买了老款的macbook pro，苹果的Logo还会发光，有USB, MiniDP, HDMI的多种接口，嗯，还有便宜。。。&lt;/p&gt;

&lt;p&gt;Hugo和git的使用感受，果断好了太多，于是又有了写博客的动力。只是mac上的Office要收费，现在的我作为一个PPTer。。。不过，I don&amp;rsquo;t care! 我是一个程序员！&lt;/p&gt;

&lt;p&gt;希望能扭转当前的状态，少一点ppt和xls, 多撸代码多写wiki。不然就太对不起我的第一台Mac了&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Oozie ssh action问题排查</title>
      <link>http://blog.ywheel.com/post/2016/07/14/oozie_ssh_action/</link>
      <pubDate>Thu, 14 Jul 2016 20:21:30 +0800</pubDate>
      
      <guid>http://blog.ywheel.com/post/2016/07/14/oozie_ssh_action/</guid>
      <description>

&lt;h2 id=&#34;问题描述&#34;&gt;问题描述&lt;/h2&gt;

&lt;p&gt;最近在我们的其中一个现网环境中部署MR程序，MR程序的调度自然是用Oozie了。在Oozie的Workflow中，我们使用ssh action登录到一台节点上，并且在该节点上部署了脚本做数据库的建表操作。&lt;/p&gt;

&lt;p&gt;该程序已经在现网多个生产环境部署运行过，经过了多次验证，但没想到在该环境中仍然出现了问题。问题出在ssh action中，并且抛出了一个&lt;code&gt;Cannot run program &amp;quot;scp&amp;quot;: error=2, No such file or directory&lt;/code&gt;的错误。&lt;/p&gt;

&lt;p&gt;具体的错误栈信息如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;2016-06-12 22:30:54,713 INFO org.apache.oozie.action.ssh.SshActionExecutor: SERVER[Master] USER[hdfs] GROUP[-] TOKEN[] APP[TestSsh] JOB[0000201-160113124428061-oozie-oozi-W] ACTION[0000201-160113124428061-oozie-oozi-W@ShellAction] Attempting to copy ssh base scripts to remote host [root@192.168.1.154]
2016-06-12 22:30:54,869 WARN org.apache.oozie.action.ssh.SshActionExecutor: SERVER[Master] USER[hdfs] GROUP[-] TOKEN[] APP[TestSsh] JOB[0000201-160113124428061-oozie-oozi-W] ACTION[0000201-160113124428061-oozie-oozi-W@ShellAction] Error while executing ssh EXECUTION
2016-06-12 22:30:54,870 WARN org.apache.oozie.command.wf.ActionStartXCommand: SERVER[Master] USER[hdfs] GROUP[-] TOKEN[] APP[TestSsh] JOB[0000201-160113124428061-oozie-oozi-W] ACTION[0000201-160113124428061-oozie-oozi-W@ShellAction] Error starting action [ShellAction]. ErrorType [ERROR], ErrorCode [UNKOWN_ERROR], Message [UNKOWN_ERROR: Cannot run program &amp;quot;scp&amp;quot;: error=2, No such file or directory]
org.apache.oozie.action.ActionExecutorException: UNKOWN_ERROR: Cannot run program &amp;quot;scp&amp;quot;: error=2, No such file or directory
    at org.apache.oozie.action.ssh.SshActionExecutor.execute(SshActionExecutor.java:599)
    at org.apache.oozie.action.ssh.SshActionExecutor.start(SshActionExecutor.java:204)
    at org.apache.oozie.command.wf.ActionStartXCommand.execute(ActionStartXCommand.java:228)
    at org.apache.oozie.command.wf.ActionStartXCommand.execute(ActionStartXCommand.java:63)
    at org.apache.oozie.command.XCommand.call(XCommand.java:281)
    at org.apache.oozie.service.CallableQueueService$CompositeCallable.call(CallableQueueService.java:323)
    at org.apache.oozie.service.CallableQueueService$CompositeCallable.call(CallableQueueService.java:252)
    at org.apache.oozie.service.CallableQueueService$CallableWrapper.run(CallableQueueService.java:174)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:744)
Caused by: java.io.IOException: Cannot run program &amp;quot;scp&amp;quot;: error=2, No such file or directory
    at java.lang.ProcessBuilder.start(ProcessBuilder.java:1041)
    at java.lang.Runtime.exec(Runtime.java:617)
    at java.lang.Runtime.exec(Runtime.java:485)
    at org.apache.oozie.action.ssh.SshActionExecutor.executeCommand(SshActionExecutor.java:332)
    at org.apache.oozie.action.ssh.SshActionExecutor.setupRemote(SshActionExecutor.java:376)
    at org.apache.oozie.action.ssh.SshActionExecutor$1.call(SshActionExecutor.java:206)
    at org.apache.oozie.action.ssh.SshActionExecutor$1.call(SshActionExecutor.java:204)
    at org.apache.oozie.action.ssh.SshActionExecutor.execute(SshActionExecutor.java:548)
    ... 10 more
Caused by: java.io.IOException: error=2, No such file or directory
    at java.lang.UNIXProcess.forkAndExec(Native Method)
    at java.lang.UNIXProcess.&amp;lt;init&amp;gt;(UNIXProcess.java:135)
    at java.lang.ProcessImpl.start(ProcessImpl.java:130)
    at java.lang.ProcessBuilder.start(ProcessBuilder.java:1022)
    ... 17 more
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;问题排查&#34;&gt;问题排查&lt;/h2&gt;

&lt;h3 id=&#34;排查oozie-server是否有scp命令&#34;&gt;排查Oozie Server是否有SCP命令&lt;/h3&gt;

&lt;p&gt;Oozie的ssh action是在oozie server所在服务器上，登录到目标机器，这样就需要做oozie server机器到目标机器的免密登录。&lt;/p&gt;

&lt;p&gt;由于我们在安装集群时，使用的是自己开发的脚本安装，而在脚本安装过程中， &lt;code&gt;scp&lt;/code&gt;是一定会使用的命令，所以Oozie Server节点应该安装有SCP的。在终端中试了一下，&lt;code&gt;scp&lt;/code&gt;是能够正常使用的。&lt;/p&gt;

&lt;h3 id=&#34;排查oozie-ssh免密登录&#34;&gt;排查Oozie ssh免密登录&lt;/h3&gt;

&lt;p&gt;看到上述错误信息，其实应该不属于ssh免密登录问题，但这个&lt;code&gt;scp&lt;/code&gt;的问题的确没有头绪（我们的脚本中并没有使用&lt;code&gt;scp&lt;/code&gt;，而Oozie Server上又可以使用&lt;code&gt;scp&lt;/code&gt;命令。）&lt;/p&gt;

&lt;p&gt;于是，我们重做了oozie server到目标机器的免密登录，但无任何效果。更换目标机器、重新部署Oozie Server到另一个节点后再做免密登录均不能解决问题。&lt;/p&gt;

&lt;h3 id=&#34;查看oozie代码寻找scp过程&#34;&gt;查看Oozie代码寻找scp过程&lt;/h3&gt;

&lt;p&gt;没辙了，只能看看Oozie的代码，看看在ssh action中，为什么需要scp, 在哪里使用到了scp.&lt;/p&gt;

&lt;p&gt;从github上下载了Oozie的源代码，在&lt;code&gt;core&lt;/code&gt;包中找到&lt;code&gt;org.apache.oozie.action.ssh.SshActionExecutor&lt;/code&gt;, 其中有一个&lt;code&gt;start&lt;/code&gt;方法，Oozie在执行ssh action时，会调用该方法：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;public void start(final Context context, final WorkflowAction action) throws ActionExecutorException {
        XLog log = XLog.getLog(getClass());
        log.info(&amp;quot;start() begins&amp;quot;);
        String confStr = action.getConf();
        Element conf;
        try {
            conf = XmlUtils.parseXml(confStr);
        }
        catch (Exception ex) {
            throw convertException(ex);
        }
        Namespace nameSpace = conf.getNamespace();
        Element hostElement = conf.getChild(&amp;quot;host&amp;quot;, nameSpace);
        String hostString = hostElement.getValue().trim();
        hostString = prepareUserHost(hostString, context);
        final String host = hostString;
        final String dirLocation = execute(new Callable&amp;lt;String&amp;gt;() {
            public String call() throws Exception {
                return setupRemote(host, context, action);
            }

        });

        String runningPid = execute(new Callable&amp;lt;String&amp;gt;() {
            public String call() throws Exception {
                return checkIfRunning(host, context, action);
            }
        });
        ...
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在&lt;code&gt;start&lt;/code&gt;方法中执行到的第一个&lt;code&gt;callable&lt;/code&gt;就是去调用&lt;code&gt;setupRemote&lt;/code&gt;方法：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;protected String setupRemote(String host, Context context, WorkflowAction action) throws IOException, InterruptedException {
        XLog log = XLog.getLog(getClass());
        log.info(&amp;quot;Attempting to copy ssh base scripts to remote host [{0}]&amp;quot;, host);
        String localDirLocation = Services.get().getRuntimeDir() + &amp;quot;/ssh&amp;quot;;
        if (localDirLocation.endsWith(&amp;quot;/&amp;quot;)) {
            localDirLocation = localDirLocation.substring(0, localDirLocation.length() - 1);
        }
        File file = new File(localDirLocation + &amp;quot;/ssh-base.sh&amp;quot;);
        if (!file.exists()) {
            throw new IOException(&amp;quot;Required Local file &amp;quot; + file.getAbsolutePath() + &amp;quot; not present.&amp;quot;);
        }
        file = new File(localDirLocation + &amp;quot;/ssh-wrapper.sh&amp;quot;);
        if (!file.exists()) {
            throw new IOException(&amp;quot;Required Local file &amp;quot; + file.getAbsolutePath() + &amp;quot; not present.&amp;quot;);
        }
        String remoteDirLocation = getRemoteFileName(context, action, null, true, true);
        String command = XLog.format(&amp;quot;{0}{1}  mkdir -p {2} &amp;quot;, SSH_COMMAND_BASE, host, remoteDirLocation).toString();
        executeCommand(command);
        command = XLog.format(&amp;quot;{0}{1}/ssh-base.sh {2}/ssh-wrapper.sh {3}:{4}&amp;quot;, SCP_COMMAND_BASE, localDirLocation,
                              localDirLocation, host, remoteDirLocation);
        executeCommand(command);
        command = XLog.format(&amp;quot;{0}{1}  chmod +x {2}ssh-base.sh {3}ssh-wrapper.sh &amp;quot;, SSH_COMMAND_BASE, host,
                              remoteDirLocation, remoteDirLocation);
        executeCommand(command);
        return remoteDirLocation;
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;由此可以看到，在&lt;code&gt;setupRemote&lt;/code&gt;方法中，Oozie会先寻找两个文件：&lt;code&gt;ssh-base.sh&lt;/code&gt;,&lt;code&gt;ssh-wrapper.sh&lt;/code&gt;, 这两个文件存在于Oozie Server运行时的目录里（Services.get().getRuntimeDir() + &amp;ldquo;/ssh&amp;rdquo;）. 在该环境的Oozie Server节点上，执行以下命令查找运行时目录：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[oozie@master oozie]$ ps aux | grep oozie
root      1068  0.0  0.0 145432  1592 pts/1    S    21:47   0:00 su oozie
oozie     1069  0.0  0.0 108300  1912 pts/1    S+   21:47   0:00 bash
root      3842  0.0  0.0 145432  1592 pts/9    S    22:18   0:00 su oozie
oozie     3843  0.0  0.0 108296  1880 pts/9    S    22:18   0:00 bash
oozie     4885  5.4  1.4 5295004 467072 ?      Sl   22:28   1:00 /usr/java/jdk1.7.0_45-cloudera/bin/java -Djava.util.logging.config.file=/var/lib/oozie/tomcat-deployment/conf/logging.properties -Djava.util.logging.manager=org.apache.juli.ClassLoaderLogManager -Xms1073741824 -Xmx1073741824 -XX:OnOutOfMemoryError=/usr/lib64/cmf/service/common/killparent.sh -Doozie.home.dir=/opt/cloudera/parcels/CDH-5.2.0-1.cdh5.2.0.p0.36/lib/oozie -Doozie.config.dir=/var/run/cloudera-scm-agent/process/3518-oozie-OOZIE_SERVER -Doozie.log.dir=/var/log/oozie -Doozie.log.file=oozie-cmf-oozie-OOZIE_SERVER-Master.log.out -Doozie.config.file=oozie-site.xml -Doozie.log4j.file=log4j.properties -Doozie.log4j.reload=10 -Doozie.http.hostname=Master -Doozie.http.port=11000 -Djava.net.preferIPv4Stack=true -Doozie.admin.port=11001 -Doozie.instance.id=Master -Dderby.stream.error.file=/var/log/oozie/derby.log -Doozie.https.keystore.file= -Doozie.https.keystore.pass= -Doozie.https.port= -Djava.endorsed.dirs=/opt/cloudera/parcels/CDH-5.2.0-1.cdh5.2.0.p0.36/lib/bigtop-tomcat/endorsed -classpath /opt/cloudera/parcels/CDH-5.2.0-1.cdh5.2.0.p0.36/lib/bigtop-tomcat/bin/bootstrap.jar -Dcatalina.base=/var/lib/oozie/tomcat-deployment -Dcatalina.home=/opt/cloudera/parcels/CDH-5.2.0-1.cdh5.2.0.p0.36/lib/bigtop-tomcat -Djava.io.tmpdir=/var/run/cloudera-scm-agent/process/3518-oozie-OOZIE_SERVER/temp org.apache.catalina.startup.Bootstrap start
oozie     6163  0.0  0.0 110228  1184 pts/9    R+   22:47   0:00 ps aux
oozie     6164  0.0  0.0 103252   900 pts/9    R+   22:47   0:00 grep oozie
root     61612  0.0  0.0 145432  1592 pts/3    S    20:54   0:00 su oozie
oozie    61613  0.0  0.0 108296  1908 pts/3    S+   20:54   0:00 bash
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;进入目录找到两个shell脚本文件：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[oozie@master temp]$ cd /var/run/cloudera-scm-agent/process/3518-oozie-OOZIE_SERVER/temp
[oozie@master temp]$ ll
总用量 0
drwxr-xr-x 3 oozie oozie 60 6月  12 22:28 oozie-oozi2411540976346867728.dir
[oozie@master temp]$ cd oozie-oozi2411540976346867728.dir/
[oozie@master oozie-oozi2411540976346867728.dir]$ ll
总用量 0
drwxr-xr-x 2 oozie oozie 80 6月  12 22:28 ssh
[oozie@master oozie-oozi2411540976346867728.dir]$ cd ssh/
[oozie@master ssh]$ ll
总用量 8
-rw-r--r-- 1 oozie oozie 1469 6月  12 22:28 ssh-base.sh
-rw-r--r-- 1 oozie oozie 2263 6月  12 22:28 ssh-wrapper.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;说明这两个文件都存在。&lt;/p&gt;

&lt;p&gt;在&lt;code&gt;setupRemote&lt;/code&gt;方法中，首先运行&lt;code&gt;ssh&lt;/code&gt;+&lt;code&gt;mkdir&lt;/code&gt;命令，在目标机器上创建目录，路径为&lt;code&gt;oozie-oozi/${WORKFLOW-ID}/${ACTION}--${ACTION-TYPE}&lt;/code&gt;。在ssh目标机器的/root目录下(由于是登录到root账号)，创建了&lt;code&gt;/root/oozie-oozi/0000201-160113124428061-oozie-oozi-W/ShellAction--ssh&lt;/code&gt;目录。该目录的创建成功说明至少免密登录没有问题，Oozie Server能够免密登录到目标机器执行命令。&lt;/p&gt;

&lt;p&gt;接下来执行&lt;code&gt;scp&lt;/code&gt;操作，将Oozie Server中的&lt;code&gt;ssh-base.sh&lt;/code&gt;和&lt;code&gt;ssh-wrapper.sh&lt;/code&gt;文件scp到目标目录中，但是这一步失败。拼接该命令为：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scp -o PasswordAuthentication=no -o KbdInteractiveDevices=no -o StrictHostKeyChecking=no -o ConnectTimeout=20 /var/run/cloudera-scm-agent/process/3518-oozie-OOZIE_SERVER/temp/oozie-oozi2411540976346867728.dir/ssh/ssh-base.sh /var/run/cloudera-scm-agent/process/-oozie-OOZIE_SERVER/temp/oozie-oozi4864889223161337030.dir/ssh/ssh-wrapper.sh root@localhost:oozie-oozi/0000201-160113124428061-oozie-oozi-W/ShellAction--ssh/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;但奇怪的是在Oozie Server所在服务器终端下切换到oozie用户，单独执行上述命令却能成功。&lt;/p&gt;

&lt;p&gt;于是，这个问题就变成了: scp其实没问题，在终端下执行命令均可成功，但是Oozie Server执行scp缺不成功。&lt;/p&gt;

&lt;h3 id=&#34;排查cdh版本问题&#34;&gt;排查CDH版本问题&lt;/h3&gt;

&lt;p&gt;想来想去，没有头绪。在网上搜也没找到想要的结果。于是想到了土办法：升级。 有时候升级就像是重装系统一样&amp;rdquo;管用&amp;rdquo;。&lt;/p&gt;

&lt;p&gt;当前的版本是CDH 5.2版本，Oozie的版本是4.0.0。而Oozie的新版本4.1.0是到了CDH 5.4才有的。由于我们在现网生产环境除了CDH 5.2之外，还使用了5.6，因此决定直接升到5.6.&lt;/p&gt;

&lt;p&gt;跨版本升级的各种问题在此不表，反正是又升CDH，又升元数据啥的，并且由于集群并非每台机器都能连外网，升级agent需要各种依赖包，各种费劲。&lt;/p&gt;

&lt;p&gt;总之一句话，升级到了5.6，仍然不能解决问题。&lt;/p&gt;

&lt;h3 id=&#34;环境变量&#34;&gt;环境变量！！！&lt;/h3&gt;

&lt;p&gt;折腾了各种办法，只有再冷静下来仔细想想。为什么相同的集群版本，相同的软件版本，在其他地方都能执行成功个，而在该环境下就不行了呢？&lt;/p&gt;

&lt;p&gt;答案就是： 环境变量。&lt;/p&gt;

&lt;p&gt;Oozie执行scp命令是使用&lt;code&gt;Runtime.exec&lt;/code&gt;，真正执行仍需要找到&lt;code&gt;scp&lt;/code&gt;命令。&lt;/p&gt;

&lt;p&gt;在Oozie Server节点上查找scp:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ which scp
/usr/local/openssh/bin/scp
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;由此猜想是否oozie server的环境变量中没有这个目录到导致找不到scp？ 于是执行如下命令，创建软连接：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@master bin]# ln -s /usr/local/openssh/bin/scp scp
[root@master bin]# ll scp
lrwxrwxrwx 1 root root 26 6月  17 01:58 scp -&amp;gt; /usr/local/openssh/bin/scp
[root@master bin]# su oozie
sh-4.1$ which scp
/usr/bin/scp
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后再重试Oozie ssh action，居然成功了！！！&lt;/p&gt;

&lt;p&gt;到目标机器的&lt;code&gt;/root/oozie-oozi/&lt;/code&gt;目录下，也能找到那两个sh文件了(执行完成后会被删除)：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@master ssh-2ecf--ssh]# ll
总用量 16
-rw-r--r-- 1 root root    6 6月  17 02:13 0000001-160617010847759-oozie-oozi-W@ssh-2ecf@2.pid
-rw-r--r-- 1 root root    0 6月  17 02:13 17950.0000001-160617010847759-oozie-oozi-W@ssh-2ecf@2.stderr
-rw-r--r-- 1 root root    9 6月  17 02:13 17950.0000001-160617010847759-oozie-oozi-W@ssh-2ecf@2.stdout
-rwxr-xr-x 1 root root 1469 6月  17 02:13 ssh-base.sh
-rwxr-xr-x 1 root root 2263 6月  17 02:13 ssh-wrapper.sh
[root@master ssh-2ecf--ssh]# pwd
/root/oozie-oozi/0000001-160617010847759-oozie-oozi-W/ssh-2ecf--ssh
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;恢复oozie&#34;&gt;恢复Oozie&lt;/h3&gt;

&lt;p&gt;本以为解决了这个问题后大功告成。没想到，接下来又是一个深渊，运行oozie作业抛如下错误：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;2016-06-17 10:06:16,950 WARN org.apache.oozie.action.hadoop.SqoopActionExecutor: SERVER[Master] USER[hdfs] GROUP[-] TOKEN[] APP[ReleaseV1_hour_20160609_01] JOB[0000004-160617010847759-oozie-oozi-W] ACTION[0000004-160617010847759-oozie-oozi-W@webByHour] Launcher exception: Could not find Yarn tags property (mapreduce.job.tags)
java.lang.RuntimeException: Could not find Yarn tags property (mapreduce.job.tags)
    at org.apache.oozie.action.hadoop.LauncherMainHadoopUtils.getChildYarnJobs(LauncherMainHadoopUtils.java:52)
    at org.apache.oozie.action.hadoop.LauncherMainHadoopUtils.killChildYarnJobs(LauncherMainHadoopUtils.java:87)
    at org.apache.oozie.action.hadoop.SqoopMain.run(SqoopMain.java:165)
    at org.apache.oozie.action.hadoop.LauncherMain.run(LauncherMain.java:39)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查找原因，说是需要升级Oozie共享库(现在集群版本已经升到CDH5.6了)，于是去CM中升级Oozie共享库，先停止Oozie, 然后安装Oozie共享库，则提示找不到yarn相关的包,需要升级CM。&lt;/p&gt;

&lt;p&gt;于是又开始升级CM, 碰到如下问题：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;libssl.so.10: cannot open shared object file: No such file or directory&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;升级CM Server和agent的时候，依赖报错：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;** Found 6 pre-existing rpmdb problem(s), &#39;yum check&#39; output follows:
git-1.7.1-3.el6_4.1.x86_64 has missing requires of openssh-clients
6:kdelibs-devel-4.3.4-20.el6_4.1.x86_64 has missing requires of openssl-devel
mysql-devel-5.1.71-1.el6.x86_64 has missing requires of openssl-devel
1:net-snmp-devel-5.5-49.el6.x86_64 has missing requires of openssl-devel
python-meh-0.12.1-3.el6.noarch has missing requires of openssh-clients
systemtap-client-2.3-3.el6.x86_64 has missing requires of openssh-clients
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;找不到正确的repo:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Error: Cannot find a valid baseurl for repo: base 
Could not retrieve mirrorlist http://mirrorlist.centos.org/?release=6&amp;amp;arch=x86_64&amp;amp;repo=os error was 
14: PYCURL ERROR 6 - &amp;quot;Couldn&#39;t resolve host &#39;mirrorlist.centos.org&#39;&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;尝试使用以下方法解决：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;rpm -ivh openssl-1.0.1e-15.el6.x86_64.rpm&lt;/code&gt;安装openssl&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;配置&lt;code&gt;/etc/yum.repos.d/cdh.repo&lt;/code&gt;,&lt;code&gt;/etc/yum.repos.d/cloudera-manager.repo&lt;/code&gt;,添加阿里云centos源&lt;code&gt;wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-6.repo&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;执行&lt;code&gt;yum upgrade &#39;cloudera-*&#39; --skip-broken&lt;/code&gt;安装cloudera server. 可能仍然会碰到一些依赖库的问题，但不影响&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;执行&lt;code&gt;yum -y install cloudera-manager-agent&lt;/code&gt;. 这个要求一定要解决依赖库的问题。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;使用上诉方法搞定了CM Server，CM界面能访问，并搞定了NameNode和SecondaryNameNode两个节点的agent，并能启动Cloudera Manager Service进行监控。&lt;/p&gt;

&lt;p&gt;在执行&lt;code&gt;yum -y install cloudera-manager-agent&lt;/code&gt;时，可能会报如下错误：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# yum -y install cloudera-manager-agent
Loaded plugins: fastestmirror, refresh-packagekit
Repository updates is listed more than once in the configuration
http://mirrors.aliyun.com/centos/6/os/x86_64/repodata/repomd.xml: [Errno 14] PYCURL ERROR 6 - &amp;quot;Couldn&#39;t resolve host &#39;mirrors.aliyun.com&#39;&amp;quot;
Trying other mirror.
http://mirrors.aliyuncs.com/centos/6/os/x86_64/repodata/repomd.xml: [Errno 14] PYCURL ERROR 6 - &amp;quot;Couldn&#39;t resolve host &#39;mirrors.aliyuncs.com&#39;&amp;quot;
Trying other mirror.
Error: Cannot retrieve repository metadata (repomd.xml) for repository: base. Please verify its path and try again
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;需要编辑/etc/resolv.conf文件成这样：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# Generated by NetworkManager
nameserver 123.125.81.6
nameserver 114.114.114.114
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后执行&lt;code&gt;yum -y install cloudera-manager-agent&lt;/code&gt;, 各种进度条之后看到：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Dependency Installed:
  MySQL-python.x86_64 0:1.2.3-0.3.c1.1.el6          mod_ssl.x86_64 1:2.2.15-53.el6.centos          openssl-devel.x86_64 0:1.0.1e-48.el6_8.1          python-psycopg2.x86_64 0:2.0.14-2.el6         

Updated:
  cloudera-manager-agent.x86_64 0:5.6.1-1.cm561.p0.3.el6                                                                                                                                            

Dependency Updated:
  cloudera-manager-daemons.x86_64 0:5.6.1-1.cm561.p0.3.el6     httpd.x86_64 0:2.2.15-53.el6.centos     httpd-devel.x86_64 0:2.2.15-53.el6.centos     httpd-tools.x86_64 0:2.2.15-53.el6.centos    
  openssl.x86_64 0:1.0.1e-48.el6_8.1                          

Complete!
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;执行&lt;code&gt;service cloudera-scm-agent start&lt;/code&gt;启动agent.&lt;/p&gt;

&lt;p&gt;随后，重新安装Oozie共享库，集群可以正常使用。测试了MR作业，均可成功。总算大功告成！&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;小结：&lt;/p&gt;

&lt;p&gt;一个&amp;rdquo;小&amp;rdquo;问题，牵扯出了如上的一大堆，前前后后排查了4天，虽不是4天都在排查这个问题，但终究得以解决。完全没想到的问题总会出现，时间预估总是太乐观，解决问题的过程总是坎坷，解决问题后的愉悦难以言表，还有，客户的要求总是那么无理，可是谁让他是&amp;rdquo;上帝&amp;rdquo;呢。&lt;/p&gt;

&lt;p&gt;还记得那天，白天干完别的活，拖着疲惫的身体回家，看到issue列表，想起来这个拖了几天还没解决而且没有头绪的问题，一直尝试弄到凌晨4点，才发现环境变量的问题，&amp;rdquo;兴奋&amp;rdquo;的差点没直接睡在电脑上。。。多么简单的解决办法，发现他却也不简单。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Hive In Oozie Workflow</title>
      <link>http://blog.ywheel.com/post/2016/06/12/hive_in_oozie_workflow/</link>
      <pubDate>Sun, 12 Jun 2016 00:16:44 +0800</pubDate>
      
      <guid>http://blog.ywheel.com/post/2016/06/12/hive_in_oozie_workflow/</guid>
      <description>

&lt;p&gt;在公司搭建和维护大数据平台，并提供给其他数据分析人员使用，hive就是那些非程序员使用的最多（几乎是唯一）的一个服务。当然，在每天的数据处理中，我们为了简化编码工作量，以及使用到数据分析人员积累的成果，可以直接使用或简单修改他们提供的hql脚本进行数据处理，并且使用Oozie调度hive作业。&lt;/p&gt;

&lt;p&gt;在此介绍一下Hive action的编写，也记录一下曾经在这方面踩到的坑。&lt;/p&gt;

&lt;h2 id=&#34;hive-action&#34;&gt;Hive Action&lt;/h2&gt;

&lt;p&gt;在Oozie的workflow配置中添加一个hive action非常简单。&lt;/p&gt;

&lt;p&gt;Hive action运行一个Hive作业, Oozie workflow将等待Hive作业运行完成后再进入下一个action。 在hive action中，需要配置诸如job-tracker, name-node, hive scripts等参数，当然在Hive action中也能配置在启动hive作业之前创建或删除HDFS目录。&lt;/p&gt;

&lt;p&gt;在Oozie workflow的hive action中，也可以支持hive脚本的参数变量，使用&lt;code&gt;${VARIABLES}&lt;/code&gt;来表示。&lt;/p&gt;

&lt;p&gt;以下是官网中对hive action的语法例子：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;workflow-app name=&amp;quot;[WF-DEF-NAME]&amp;quot; xmlns=&amp;quot;uri:oozie:workflow:0.1&amp;quot;&amp;gt;
    ...
    &amp;lt;action name=&amp;quot;[NODE-NAME]&amp;quot;&amp;gt;
        &amp;lt;hive xmlns=&amp;quot;uri:oozie:hive-action:0.2&amp;quot;&amp;gt;
            &amp;lt;job-tracker&amp;gt;[JOB-TRACKER]&amp;lt;/job-tracker&amp;gt;
            &amp;lt;name-node&amp;gt;[NAME-NODE]&amp;lt;/name-node&amp;gt;
            &amp;lt;prepare&amp;gt;
               &amp;lt;delete path=&amp;quot;[PATH]&amp;quot;/&amp;gt;
               ...
               &amp;lt;mkdir path=&amp;quot;[PATH]&amp;quot;/&amp;gt;
               ...
            &amp;lt;/prepare&amp;gt;
            &amp;lt;job-xml&amp;gt;[HIVE SETTINGS FILE]&amp;lt;/job-xml&amp;gt;
            &amp;lt;configuration&amp;gt;
                &amp;lt;property&amp;gt;
                    &amp;lt;name&amp;gt;[PROPERTY-NAME]&amp;lt;/name&amp;gt;
                    &amp;lt;value&amp;gt;[PROPERTY-VALUE]&amp;lt;/value&amp;gt;
                &amp;lt;/property&amp;gt;
                ...
            &amp;lt;/configuration&amp;gt;
            &amp;lt;script&amp;gt;[HIVE-SCRIPT]&amp;lt;/script&amp;gt;
            &amp;lt;param&amp;gt;[PARAM-VALUE]&amp;lt;/param&amp;gt;
                ...
            &amp;lt;param&amp;gt;[PARAM-VALUE]&amp;lt;/param&amp;gt;
            &amp;lt;file&amp;gt;[FILE-PATH]&amp;lt;/file&amp;gt;
            ...
            &amp;lt;archive&amp;gt;[FILE-PATH]&amp;lt;/archive&amp;gt;
            ...
        &amp;lt;/hive&amp;gt;
        &amp;lt;ok to=&amp;quot;[NODE-NAME]&amp;quot;/&amp;gt;
        &amp;lt;error to=&amp;quot;[NODE-NAME]&amp;quot;/&amp;gt;
    &amp;lt;/action&amp;gt;
    ...
&amp;lt;/workflow-app&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;介绍一下这个语法中有几个参数：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;code&gt;prepare&lt;/code&gt; 如果需要在hive作业之前创建或删除HDFS目录，则可以增加&lt;code&gt;prepare&lt;/code&gt;参数，指定需要创建或删除的HDFS路径。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;job-xml&lt;/code&gt; 指定hive-site.xml所在HDFS上的路径；如果是CDH搭建的集群，则可以在任何一台hive gateway机器上的&lt;code&gt;/etc/hive/conf&lt;/code&gt;目录下找到该配置文件。如果不指定该文件路径，hive action就不work。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;configuration&lt;/code&gt; 包含传递给hive作业的参数，可以没有这个配置项，这样就全部使用默认配置&lt;/li&gt;
&lt;li&gt;&lt;code&gt;script&lt;/code&gt; 指定hql脚本所在HDFS上的路径；这个参数是hive action必须的。这个hql脚本中，可以使用&lt;code&gt;${VARIABLES}&lt;/code&gt;来表示参数，获取在hive action中定义的&lt;code&gt;param&lt;/code&gt;参数配置&lt;/li&gt;
&lt;li&gt;&lt;code&gt;param&lt;/code&gt; 定义在hql脚本中所需要的变量值&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;如下是我在生产环境中使用hive action的一个样例：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;    &amp;lt;action name=&amp;quot;HiveAction&amp;quot;&amp;gt;
        &amp;lt;hive xmlns=&amp;quot;uri:oozie:hive-action:0.2&amp;quot;&amp;gt;
            &amp;lt;job-tracker&amp;gt;${jobTracker}&amp;lt;/job-tracker&amp;gt;
            &amp;lt;name-node&amp;gt;${nameNode}&amp;lt;/name-node&amp;gt;
            &amp;lt;!-- Need to upload hive-site.xml file to HDFS from local disk (/etc/hive/conf) first
            --&amp;gt;
            &amp;lt;job-xml&amp;gt;${HDFS_PREFIX}/file/xml/hive-site.xml&amp;lt;/job-xml&amp;gt;
            &amp;lt;!--
            TODO: Maybe we need to delete the old path here
            &amp;lt;prepare&amp;gt;
                &amp;lt;delete path=&amp;quot;${jobOutput}&amp;quot;/&amp;gt;
            &amp;lt;/prepare&amp;gt;
            --&amp;gt;
            &amp;lt;!--
            &amp;lt;configuration&amp;gt;
                &amp;lt;property&amp;gt;
                    &amp;lt;name&amp;gt;mapred.compress.map.output&amp;lt;/name&amp;gt;
                    &amp;lt;value&amp;gt;true&amp;lt;/value&amp;gt;
                &amp;lt;/property&amp;gt;
            &amp;lt;/configuration&amp;gt;
            --&amp;gt;
            &amp;lt;script&amp;gt;${HDFS_PREFIX}/file/hql/hive_query.hql&amp;lt;/script&amp;gt;
            &amp;lt;param&amp;gt;inputLogDay=${inputLogDay}&amp;lt;/param&amp;gt;
            &amp;lt;param&amp;gt;inputPath=${OUTPUT_DIR}&amp;lt;/param&amp;gt;
            &amp;lt;param&amp;gt;hiveDBname=${hiveDBname}&amp;lt;/param&amp;gt;
        &amp;lt;/hive&amp;gt;
        &amp;lt;ok to=&amp;quot;end&amp;quot;/&amp;gt;
        &amp;lt;error to=&amp;quot;kill&amp;quot;/&amp;gt;
    &amp;lt;/action&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;output-data-exceeds-its-limit&#34;&gt;Output data exceeds its limit&lt;/h2&gt;

&lt;p&gt;在上面的样例中可以看到，oozie workflow中的hive action不关心hql文件中定义的hive查询逻辑，在oozie workflow中做到了尽量简单，而hive的逻辑正确性保障和作业执行成功的保障都需要hql本身来完成。&lt;/p&gt;

&lt;p&gt;我就在生产环境就碰到了个诡异的问题： &lt;code&gt;org.apache.oozie.action.hadoop.LauncherException: Output data exceeds its limit [2048]&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;查找了一下原因，Oozie默认最大输出的数据大小为2K，即2048B；而在hive action中，执行hql脚本时提交的MR作业的ID（比如job_1464936467641_1657）均会被记录下来并且返回给Oozie，如果在一个hql脚本中，hive查询语句过多将会导致Oozie收到的结果数据超过2k大小，于是就抛这个错误。解决办法, 添加如下配置到Oozie-site.xml中，重启Oozie服务生效：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;oozie.action.max.output.data&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;204800&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;PS：如果是CDH搭建的集群，那么可以在 &lt;code&gt;集群-&amp;gt;Oozie-&amp;gt;配置-&amp;gt;Oozie Server Default Group-&amp;gt;高级-&amp;gt;oozie-site.xml 的 Oozie Server 高级配置代码段（安全阀）&lt;/code&gt;中添加上述配置。&lt;/p&gt;

&lt;p&gt;PS: 尝试过在Hive action中的&lt;code&gt;configuration&lt;/code&gt;参数中添加这样的配置，但经过测试发现并不work.&lt;/p&gt;

&lt;h2 id=&#34;outofmemoryerror&#34;&gt;OutOfMemoryError&lt;/h2&gt;

&lt;p&gt;当执行一个hql脚本时，脚本中包含多个查询语句（好吧，又是好几百个），其中每个语句都经过测试能够正常运行并成功结束，但是放在一起被Oozie调用后缺抛如下错误：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Launching Job 613 out of 857
Number of reduce tasks is set to 0 since there&#39;s no reduce operator
java.lang.OutOfMemoryError: Java heap space
    at org.apache.hadoop.hdfs.util.ByteArrayManager$NewByteArrayWithoutLimit.newByteArray(ByteArrayManager.java:308)
    at org.apache.hadoop.hdfs.DFSOutputStream.createPacket(DFSOutputStream.java:192)
    at org.apache.hadoop.hdfs.DFSOutputStream.writeChunk(DFSOutputStream.java:1883)
    at org.apache.hadoop.fs.FSOutputSummer.writeChecksumChunks(FSOutputSummer.java:206)
    at org.apache.hadoop.fs.FSOutputSummer.write1(FSOutputSummer.java:124)
    at org.apache.hadoop.fs.FSOutputSummer.write(FSOutputSummer.java:110)
    at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:58)
    at java.io.DataOutputStream.write(DataOutputStream.java:107)
    at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:87)
    at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:59)
    at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:119)
    at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:366)
    at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:338)
    at org.apache.hadoop.fs.FileSystem.copyFromLocalFile(FileSystem.java:1905)
    at org.apache.hadoop.fs.FileSystem.copyFromLocalFile(FileSystem.java:1873)
    at org.apache.hadoop.fs.FileSystem.copyFromLocalFile(FileSystem.java:1838)
    at org.apache.hadoop.mapreduce.JobSubmitter.copyJar(JobSubmitter.java:375)
    at org.apache.hadoop.mapreduce.JobSubmitter.copyAndConfigureFiles(JobSubmitter.java:256)
    at org.apache.hadoop.mapreduce.JobSubmitter.copyAndConfigureFiles(JobSubmitter.java:390)
    at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:483)
    at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1306)
    at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1303)
    at java.security.AccessController.doPrivileged(Native Method)
    at javax.security.auth.Subject.doAs(Subject.java:415)
    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
    at org.apache.hadoop.mapreduce.Job.submit(Job.java:1303)
    at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:564)
    at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:559)
    at java.security.AccessController.doPrivileged(Native Method)
    at javax.security.auth.Subject.doAs(Subject.java:415)
    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
    at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:559)
FAILED: Execution Error, return code -101 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask. Java heap space
MapReduce Jobs Launched: 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从上述日志分析，这个hql包含很多个job，在执行过程中失败（&lt;code&gt;Launching Job 613 out of 857&lt;/code&gt;）. 直接解决这个问题的方式就是：加大内存。我们知道，Oozie实现hive action的方式为先启动一个launcher（一个只有Map的Job），即client，用于提交hive任务；实际上进行数据处理的job是hive提交的MR作业。在这个错误中，是launcher发生了&lt;code&gt;OutOfMemoryError&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;解决办法也很简单，在hive action的&lt;code&gt;configuration&lt;/code&gt;中添加如下配置，增大launcher的内存：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;configuration&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;oozie.launcher.mapreduce.map.memory.mb&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;4096&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;oozie.launcher.mapreduce.map.java.opts&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;-Xmx3400m&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
&amp;lt;/configuration&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其实把这些参数的&lt;code&gt;oozie.launcher&lt;/code&gt;前缀去掉就是Hadoop的参数配置，Oozie在提交Launcher作业时，会将这些参数传递给YARN。&lt;/p&gt;

&lt;p&gt;当然，root cause还是hql脚本没有经过优化，一个脚本包含的查询太多。深入理解业务后，精简hive查询，优化hql脚本，合理设计Oozie workflow才是正确的解决方案。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>高三楼</title>
      <link>http://blog.ywheel.com/post/2016/06/05/examination/</link>
      <pubDate>Sun, 05 Jun 2016 20:54:11 +0800</pubDate>
      
      <guid>http://blog.ywheel.com/post/2016/06/05/examination/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://o75oehjrs.bkt.clouddn.com/image/blog/%E5%A5%8B%E6%96%97.jpg?watermark/2/text/YmxvZy55d2hlZWwuY24=/font/5b6u6L2v6ZuF6buR/fontsize/500/fill/I0Y1RUZFRg==/dissolve/100/gravity/SouthEast/dx/10/dy/10&#34; alt=&#34;奋斗&#34; /&gt;&lt;/p&gt;

&lt;p&gt;明天又是一年高考了，转眼间我的高考已经过去十年。十年前，我们还是一群无知的小P孩，而十年后，我们或在北上广深打拼，我们或在老家开创天地，我们或在大洋彼岸思念故乡，而又有一群小P孩要自己把握命运。虽说高考并不十分公平，但至少给了我们机会，从小地方到大都市，从家乡土话到京腔甚至life translated。再过十年，人生又大不相同，而十年之前，就在那高三楼里为现在的自己艰苦卓绝着。。。&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;在那一年，其实也没感觉它破，本来，高三的生活是艰苦的，然后那时的教室环境跟这个生活的状态还很和谐。&lt;/p&gt;

&lt;p&gt;印象中黑板就是在水泥墙上刷一块黑漆，前后黑板都是。黑板檫从来都是檫不干净黑板，黑板已经是到处布满裂缝，出板报也给我们带来相当的难度。到高考之前的最后一次板报，干脆就直接贴了。在高考前100天左右的时候，黑板报贴满了我们的高考宣言，上晚自习，后门开着的时候，风吹进来，吹的贴在黑板报上的高考宣言哗哗的响，似乎也要吹起我们的梦想。&lt;/p&gt;

&lt;p&gt;没锁的后门，对着没玻璃的纸窗户，还记得考完最后一次去教室就是从后门进去的。高三的时候，我们经常拿后门当篮筐，拿着乒乓往门和墙围成的空隙投篮，或者拿着乒乓做着比如胯下换手扣篮这样的动作，这几乎跟跳起来摸电扇一样成为了我们班的经典课外活动，甚至直接拿着篮球去投篮，直到把日光灯打下来。说到摸电扇，那是相当的怀念啊~~把两边的桌子稍微挪挪，空出一个跑道，然后冲，跳，在家也是摸电扇练弹跳的，可惜到大学弹跳就荒废了…不过还得注意脚底下，地板是水泥的，年代久了，自然到处坑坑洼洼。那个时候两个星期一次换位置，经常换到那种要去捡块石片垫垫才能把桌子放稳的地方，换坐位搬桌子那场面，相当的壮观啊…&lt;/p&gt;

&lt;p&gt;刚上高中的时候，学校在每个教室新安装了音响系统，结束了英语老师提着放音机上课的历史。然后学校领导总在跟我们说这个东西花了学校很多钱，要好好保护。于是好好的音响放着，我们都不敢动。于是班上决定去买个放音机自己放听力，可是没想到的是居然买来之后就一直坏在那，修也修不好，各种郁闷。放音机放在班上一张多余的桌子上，每次考试时由于考场安排问题，每个班都要搬出几张桌子到其他考场，于是那个满是粉笔灰的桌子光荣的成为了考试专用桌。用这张桌子的哥们肯定很郁闷，他吸进去的粉笔灰肯定比他考的分多~~&lt;/p&gt;

&lt;p&gt;教室外的走廊，是老班经常出没的地方。上课老走神的我经常可以看到窗外的老班也在看着我，然后我就假装着应和着老师的讲课避开老班的视线，隔一会又不自主的偷偷看窗外，可是总能又看见他又偷偷的看着我…那时我们班相当活跃，基本上一下课，总会有很多同学跑到走廊上去吹风看风景，跟隔壁另一个实验班形成很鲜明的对比，走廊的一边站满了闲人，另一边却一个人也没有。高三楼两边分别是男女厕所，于是在走廊上就可以看见那些匆匆上厕所的人。经常在还没开始上课或者放学之后，站在那看着路上满满的自行车和车上的女孩…走廊前面有一些树，树的高度貌似已经超过了高三楼，有时阳光洒过来，被树挡了大半，却始终挡不了老班从对面办公室看过来的目光。他总是那么准的知道，谁谁谁今天上早读又迟到了…&lt;/p&gt;

&lt;p&gt;高三楼的后面也有树，而且是大树。好像还钉了牌子说是国家保护的树，树上经常有很多的鸟，印象中在某次考试之前还在那看见了乌鸦。树下面就是我们停自行车的地方了。那时自行车被我们叫成“铁”，铁们都停在了铁架下面，因为车棚没有顶，只剩下断了脚的铁架，于是那大树就成了车棚顶。在下雨的时候，雨水透过“车棚”把我们的铁淋个透，有时还从车棚带点乳白色粘乎乎的东西下来。那时下雨，不仅是铁们遭殃，在我们教室天花板的后面一个角，也会跟着雨天而变得湿湿的，不过比隔壁实验班拿个桶或盆啊什么的来盛从天花板上滴下的雨强点，只是让我们担心那个角会不会由于哪天太湿了而支持不住天花板。&lt;/p&gt;

&lt;p&gt;于是为了降低这种可能，学校在某天决定把高三楼顶上的淤泥清理一下来减轻那个角的负担。然后各种积累在高三楼顶的东西被工人们一铲子一铲子的扔下来。天上真的掉东西了，只是掉的不是黄金，也掉的不是地方，我们的铁们就这样查点被天上掉的东西活埋了。鸟们在树上叫着，似乎在说比起这些，它们往我们的铁上掉的东西好多了…&lt;/p&gt;

&lt;p&gt;也许是那些天上飞来之物很有营养，让各种飞虫繁衍的飞快。于是在之后的某天夜自习的时候同学一打开窗，飞虫像水利工程泄洪一样拥进来，然后就是女生尖叫的声音，还有某些硬甲虫撞击日光灯的声音。高三楼的日光灯都是在天花板上吊两跟绳下来吊着，灯座是木制的，起辉器和线很乱，上面结满了蜘蛛网。蜘蛛网还成了灭飞虫的功丞，网住了很多飞虫。不过还是不能阻止飞虫们以平均四五秒就有一只降落在你身上的情形。于是我们买来蚊香，买来灭蚊剂。用蚊香的结果是自习的时候宛如在仙境，到处飘着烟。还有次把全班同学叫出去，然后关上门拿着灭蚊剂一阵狂喷，结果是连续几天我们都是踩着一层的“尸体”怀着很深的愧疚感在上课，阿门…&lt;/p&gt;

&lt;p&gt;高考就要来了，教室里贴满了我写的激励的标语，后面黑板报两边是老班写的对联。在这样的激励下，我们最终还是迎来了高考。那时每考完一科都要回去到高三楼小小的集合一下。高考每年都有意外，而我们那年是很多意外。考完数学之后，年级主任站在高三楼的楼梯口等待着回去的同学们，他从我们脸上看到的只有失落，我们从他脸上看到的，也只有失落…&lt;/p&gt;

&lt;p&gt;不管怎样，高三就在高三楼里度过了，高三楼，见证了我们的成长，见证了我们的失落，见证了我们的欢喜，也一样见证了我们的泪水。我们走后，下一届的高三接着搬了进来，开始了他们在高三楼的高三…&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>HUE Introduction and Contribution</title>
      <link>http://blog.ywheel.com/post/2016/05/29/hue_introduction/</link>
      <pubDate>Sun, 29 May 2016 21:31:44 +0800</pubDate>
      
      <guid>http://blog.ywheel.com/post/2016/05/29/hue_introduction/</guid>
      <description>

&lt;p&gt;前段时间给同事们做了一次HUE入门使用的培训，就顺便整理出来。本篇文章先简单介绍HUE，再介绍如何给HUE贡献代码。&lt;/p&gt;

&lt;h2 id=&#34;hue是什么&#34;&gt;HUE是什么&lt;/h2&gt;

&lt;p&gt;HUE=&lt;strong&gt;Hadoop User Experience&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Hue是一个开源的Apache Hadoop UI系统，由Cloudera Desktop演化而来，最后Cloudera公司将其贡献给Apache基金会的Hadoop社区，它是基于Python Web框架Django实现的。&lt;/p&gt;

&lt;p&gt;通过使用Hue我们可以在浏览器端的Web控制台上与Hadoop集群进行交互来分析处理数据，例如操作HDFS上的数据，运行MapReduce Job，执行Hive的SQL语句，浏览HBase数据库等等。&lt;/p&gt;

&lt;h2 id=&#34;hue链接&#34;&gt;HUE链接&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Site: &lt;a href=&#34;http://gethue.com/&#34;&gt;http://gethue.com/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Github: &lt;a href=&#34;https://github.com/cloudera/hue&#34;&gt;https://github.com/cloudera/hue&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Reviews: &lt;a href=&#34;https://review.cloudera.org&#34;&gt;https://review.cloudera.org&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;核心功能&#34;&gt;核心功能&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;SQL编辑器，支持Hive, Impala, MySQL, Oracle, PostgreSQL, SparkSQL, Solr SQL, Phoenix&amp;hellip;&lt;/li&gt;
&lt;li&gt;搜索引擎Solr的各种图表&lt;/li&gt;
&lt;li&gt;Spark和Hadoop的友好界面支持&lt;/li&gt;
&lt;li&gt;支持调度系统Apache Oozie，可进行workflow的编辑、查看&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;HUE提供的这些功能相比Hadoop生态各组件提供的界面更加友好，但是一些需要debug的场景可能还是需要使用原生系统才能更加深入的找到错误的原因。&lt;/p&gt;

&lt;p&gt;HUE中查看Oozie workflow时，也可以很方便的看到整个workflow的DAG图，不过在最新版本中已经将DAG图去掉了，只能看到workflow中的action列表和他们之间的跳转关系，想要看DAG图的仍然可以使用oozie原生的界面系统查看。&lt;/p&gt;

&lt;h3 id=&#34;hue登录&#34;&gt;HUE登录&lt;/h3&gt;

&lt;p&gt;如果自己搭建了HUE，则可以使用管理员账户创建一个新的用户，然后使用新的用户进行登录，见下图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://o75oehjrs.bkt.clouddn.com/image/blog/HUE%E7%99%BB%E5%BD%95.png?watermark/2/text/YmxvZy55d2hlZWwuY24=/font/5a6L5L2T/fontsize/500/fill/Izk3QjhGMw==/dissolve/100/gravity/SouthEast/dx/10/dy/10&#34; alt=&#34;HUE登录图&#34; /&gt;&lt;/p&gt;

&lt;p&gt;使用&lt;a href=&#34;http://gethue.com/&#34;&gt;HUE官网&lt;/a&gt;上的live demo可以尝鲜。如果大家自己没有搭建大数据平台，没有安装HUE的话，可以先在该demo上尝试。点击&lt;a href=&#34;http://demo.gethue.com/&#34;&gt;Play with the live Demo now!&lt;/a&gt;,将会进入HUE的&amp;rdquo;我的文档&amp;rdquo;：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://o75oehjrs.bkt.clouddn.com/image/blog/HUE%20Demo.png?watermark/2/text/YmxvZy55d2hlZWwuY24=/font/5a6L5L2T/fontsize/500/fill/Izk3QjhGMw==/dissolve/100/gravity/SouthEast/dx/10/dy/10&#34; alt=&#34;HUE Demo图&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;hdfs文件浏览&#34;&gt;HDFS文件浏览&lt;/h3&gt;

&lt;p&gt;HUE可以很方便的浏览HDFS中的目录和文件，并且进行文件和目录的创建、复制、删除、下载以及修改权限等操作。&lt;/p&gt;

&lt;p&gt;HDFS实现了一个和POSIX系统类似的文件和目录的权限模型。每个文件和目录有一个所有者（owner）和一个组（group）。文件或目录对其所有者、同组的其他用户以及所有其他用户分别有着不同的权限。&lt;strong&gt;但，用户身份机制对HDFS本身来说只是外部特性。HDFS并不提供创建用户身份、创建组或处理用户凭证等功能。&lt;/strong&gt; 使用HUE访问HDFS时，HDFS简单的将HUE上的用户名和组的名称进行权限的校验。&lt;/p&gt;

&lt;p&gt;在Live Demo中,点击&amp;rdquo;文件浏览器&amp;rdquo;, 进入HDFS的家目录：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://o75oehjrs.bkt.clouddn.com/image/blog/HUE%20File%20Browser.png?watermark/2/text/YmxvZy55d2hlZWwuY24=/font/5a6L5L2T/fontsize/500/fill/Izk3QjhGMw==/dissolve/100/gravity/SouthEast/dx/10/dy/10&#34; alt=&#34;HUE HDFS图&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;PS:&lt;/strong&gt; Live Demo中禁了文件上传功能。&lt;/p&gt;

&lt;h3 id=&#34;作业浏览&#34;&gt;作业浏览&lt;/h3&gt;

&lt;p&gt;点击Job Browser，可以查看作业列表，并且可以通过点击右上角的&amp;rdquo;成功&amp;rdquo;,&amp;ldquo;正在运行&amp;rdquo;,&amp;ldquo;失败&amp;rdquo;,&amp;ldquo;停止&amp;rdquo;来筛选不同状态的作业：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://o75oehjrs.bkt.clouddn.com/image/blog/HUE%20Job%20Browser.png?watermark/2/text/YmxvZy55d2hlZWwuY24=/font/5a6L5L2T/fontsize/500/fill/Izk3QjhGMw==/dissolve/100/gravity/SouthEast/dx/10/dy/10&#34; alt=&#34;HUE Job Browser&#34; /&gt;&lt;/p&gt;

&lt;p&gt;我们在实际工作中发现，当集群(CDH5.2) 配置了HA后，当active的ResourceManager自动切换后(比如NN1上的ResourceManager是active，而NN2是standby，当NN1出现故障， NN2上的ResourceManager转变为active状态)，HUE的job browser将不能够正确显示。只有当修复故障后，将NN1上的ResourceManager重新变成active状态，HUE的job browser才能正常工作。不知道这个问题在后续版本是否已经得到修复。&lt;/p&gt;

&lt;h3 id=&#34;hive查询&#34;&gt;Hive查询&lt;/h3&gt;

&lt;p&gt;HUE的beeswax app提供友好方便的Hive查询功能，能够选择不同的Hive数据库，编写HQL语句，提交查询任务，并且能够在界面下方看到查询作业运行的日志。在得到结果后，还提供进行简单的图表分析能力。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://o75oehjrs.bkt.clouddn.com/image/blog/HUE%20Hive.png?watermark/2/text/YmxvZy55d2hlZWwuY24=/font/5a6L5L2T/fontsize/500/fill/Izk3QjhGMw==/dissolve/100/gravity/SouthEast/dx/10/dy/10&#34; alt=&#34;HUE Hive查询&#34; /&gt;&lt;/p&gt;

&lt;p&gt;点击&amp;rdquo;Data Browsers&amp;rdquo;-&amp;gt;&amp;ldquo;Metastore表&amp;rdquo;，还可以看到Hive中的数据库，数据库中的表以及各个表的元数据等信息。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://o75oehjrs.bkt.clouddn.com/image/blog/HUE%20Hive%20MetaStore.png?watermark/2/text/YmxvZy55d2hlZWwuY24=/font/5a6L5L2T/fontsize/500/fill/Izk3QjhGMw==/dissolve/100/gravity/SouthEast/dx/10/dy/10&#34; alt=&#34;HUE Hive MetaStore&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;oozie-workflow编辑&#34;&gt;Oozie Workflow编辑&lt;/h3&gt;

&lt;p&gt;HUE也提供了很好的Oozie的集成，能够在HUE上创建和编辑Bundles, Coordinator, Workflow. Oozie的介绍可以去&lt;a href=&#34;https://oozie.apache.org/&#34;&gt;官网&lt;/a&gt;查看。下图为在HUE上创建一个新的workflow，在该界面上，可以直接拖动不同的组件，变成DAG中的节点，并且设置各个action的流转逻辑。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://o75oehjrs.bkt.clouddn.com/image/blog/HUE%20Workflow%20Editor.png?watermark/2/text/YmxvZy55d2hlZWwuY24=/font/5a6L5L2T/fontsize/500/fill/Izk3QjhGMw==/dissolve/100/gravity/SouthEast/dx/10/dy/10&#34; alt=&#34;HUE WF Editor&#34; /&gt;&lt;/p&gt;

&lt;p&gt;当然Oozie也可以通过命令行的方式提交B,C,W. 不过是使用HUE创建的workflow，或者是通过命令行提交的workflow，都可以在HUE上查看运行的状况：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://o75oehjrs.bkt.clouddn.com/image/blog/HUE%20Workflow%20Browser.png?watermark/2/text/YmxvZy55d2hlZWwuY24=/font/5a6L5L2T/fontsize/500/fill/Izk3QjhGMw==/dissolve/100/gravity/SouthEast/dx/10/dy/10&#34; alt=&#34;HUE WF Browser&#34; /&gt;&lt;/p&gt;

&lt;p&gt;只是通过命令行提交的workflow就不可以在HUE上进行编辑了。使用配置文件、命令行提交的方式能够保证在生产环境上运行的和在测试环境上运行的版本一致，而使用HUE界面编辑的方式虽然方便，但也可能会带来人工操作在生产环境中失误的风险，有利也有弊吧。&lt;/p&gt;

&lt;h2 id=&#34;contribution&#34;&gt;Contribution&lt;/h2&gt;

&lt;p&gt;我在给同事准备培训材料的时候，到HUE的github上去查找资料。在看到HUE的主要功能时，github上的原文是这样的：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://o75oehjrs.bkt.clouddn.com/image/blog/HUE%20Features%20old.png?watermark/2/text/YmxvZy55d2hlZWwuY24=/font/5a6L5L2T/fontsize/500/fill/Izk3QjhGMw==/dissolve/100/gravity/SouthEast/dx/10/dy/10&#34; alt=&#34;HUE Features old&#34; /&gt;&lt;/p&gt;

&lt;p&gt;恰好我司主要使用的数据库是PostgreSQL，看到PostGresl感觉怪怪的，于是Google了一把，PostgreSQL有两个名字：PostgreSQL和Postgres，目前&lt;a href=&#34;https://www.postgresql.org/&#34;&gt;官方网站&lt;/a&gt;上的名字仍然是PostgreSQL. 不管PostGresl是否有什么典故，但是PostgreSQL一定是对的。因此，我去查了下如何给HUE提交代码修改。在Github上能找到wiki: &lt;a href=&#34;https://github.com/cloudera/hue/wiki/Contribute-to-HUE&#34;&gt;Contribute to HUE&lt;/a&gt;, HUE有自己的JIRA和Review Board, 但也说了&lt;code&gt;The Hue project gladly welcomes any patches or pull requests!&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;于是我在github上给HUE发了一个&lt;a href=&#34;https://github.com/cloudera/hue/issues/371&#34;&gt;Issue&lt;/a&gt;和一个&lt;a href=&#34;https://github.com/cloudera/hue/pull/372&#34;&gt;Pull Request&lt;/a&gt;。几天后Pull Request被接收，merge到了master分支上，可以看到这个&lt;a href=&#34;https://github.com/cloudera/hue/commit/61e80b3cd2820c68f2103e8cef34d50734f02c09&#34;&gt;Commit&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;在这里记录一下更新的步骤：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Fork HUE的工程，比如 &lt;a href=&#34;https://github.com/ywheel/hue&#34;&gt;ywheel/hue&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;创建一个新的分支，不要使用master分支提交修改。比如我创建了&lt;a href=&#34;https://github.com/ywheel/hue/tree/fix-postgresql-spelling&#34;&gt;fix-postgresql-spelling&lt;/a&gt;分支。&lt;/li&gt;
&lt;li&gt;将代码pull下来，修改后commit，提交到&lt;a href=&#34;https://github.com/ywheel/hue/tree/fix-postgresql-spelling&#34;&gt;fix-postgresql-spelling&lt;/a&gt;分支。&lt;/li&gt;
&lt;li&gt;创建issue。当HUE的工程上创建&lt;a href=&#34;https://github.com/cloudera/hue/issues/371&#34;&gt;issue&lt;/a&gt;, 描述清楚问题，提交。&lt;/li&gt;
&lt;li&gt;点击&amp;rsquo;Pull Request&amp;rsquo;, 选择目的工程和分支，比如cloudera/hue的master分支。填写comment, 说明已创建的issue, create pull request.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;接下来就是等了，等该提交被review, 被merge到master分支, 等你自己的名字出现在&lt;a href=&#34;https://github.com/cloudera/hue/graphs/contributors&#34;&gt;Contributors&lt;/a&gt;里面, &lt;strong&gt;then everything DONE!&lt;/strong&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;ps: 虽然改了个单词拼写就出来说简直是丢人，不过算是一个good start, 希望能在不久的将来真的能给开源项目（特别是流行的大数据生态中的开源项目）贡献代码，加油!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>老家</title>
      <link>http://blog.ywheel.com/post/2016/05/18/family/</link>
      <pubDate>Wed, 18 May 2016 02:41:01 +0800</pubDate>
      
      <guid>http://blog.ywheel.com/post/2016/05/18/family/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://o75oehjrs.bkt.clouddn.com/image/blog/%E8%80%81%E5%AE%B6.jpg?watermark/2/text/YmxvZy55d2hlZWwuY24=/font/5b6u6L2v6ZuF6buR/fontsize/500/fill/I0Y1RUZFRg==/dissolve/100/gravity/SouthEast/dx/10/dy/10&#34; alt=&#34;老家门前的三棵枣树&#34; /&gt;&lt;/p&gt;

&lt;p&gt;图：老家门前的三棵枣树&lt;/p&gt;

&lt;p&gt;5年前的今天，老家拆除了，在老家的原址上，早已盖起一栋新楼。这边文章就是在老家拆除后的几天写的，用来纪念我们心中的&amp;rdquo;老家&amp;rdquo;。&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;老家在经历了几十年的风雨后，终于到了该退休的年龄。爸爸在每当下大雨时就担心老家会不会扛不住，这种担心也在前两天彻底结束。昨天给家里通电话，得知老家已经拆的只剩下半身高的红砖墙，紧接着红砖墙也会被拆掉，在老家的那片土地上会重新盖起一栋新楼。&lt;/p&gt;

&lt;p&gt;老家，也只能是记忆中的老家了。&lt;/p&gt;

&lt;p&gt;童年有很多时间在老家度过。甚至上了小学后，只要放假不上课，还经常闹着要回老家。在那里可以玩一些很有意思的事情。还记得门口的那块大石头，常年经受水滴的拍击，在石头的表面凹进去很大一块，也许这就是“水滴石穿”的典例吧。八仙桌的竹椅子，被装上滚轮的改装摇篮，门口地里的蚯蚓，菜园里的土青蛙，屋檐下的打谷机，还有门口的三棵枣树，守护红枣的水枪。。。小时候的玩意儿还是很丰富多彩。&lt;/p&gt;

&lt;p&gt;尤其是那三棵枣树，每当暑假，红枣挂满枝头，爷爷奶奶都要召集家中所有的小孩回到老家打红枣。长辈们用长竹竿敲打树枝，红枣自然会掉下来，然后我们小孩就负责从地上把红枣捡回来，就算经常被“天上”掉下来的红枣砸到头，也不亦乐乎地忙活着捡红枣。邻居家的小孩就站在旁边看着，吞着口水。偶尔有一两颗红枣敲好掉到他们跟前，他们就立马捡起来就跑。上小学的时候，枣树上结的红枣还是挺多的，一般都能打下两箩筐满满的红枣，也会分给隔壁邻居一些。&lt;/p&gt;

&lt;p&gt;“古有‘先生不知何许人也，亦不详其姓字，宅边有五柳树，因以为号焉’，我家门口有三棵枣树，好读书，更要好好读书。。。”这是爷爷指导我们写的作文。爷爷是革命年代的读书人，学的国文，善于吟诗作赋，对对子，写的一手好毛笔字，另外还是一位“江湖郎中”，有一套治疗胃病的特效药方。我小时候，经常会有人不远千里从隔壁隔壁县一直打听，直到打听到老家里，那时电话远没有普及，所以病人只能到处托人打听，药方确实特效，所以爷爷也声名远扬。也经常会有村里的不识字的老人，拿着儿女寄来的信件找爷爷读，然后回信。爷爷对我们很好，毛笔字也是经过他的启蒙，他很享受教育孙辈的感觉，他还指导我们写过一首诗，叫《电灯》：有电真方便，开灯亮堂堂。公孙在一起，读书写文章。
小学四年级后，我们开始自己骑车。于是每次回老家，我家四口人就骑着四辆自行车浩浩荡荡得骑上大半小时。每次在老家聚集，门口都停满了自行车。从四姑父买了全家第一辆摩托车，才开始逐渐改变交通工具被腿和自行车垄断的状况。&lt;/p&gt;

&lt;p&gt;上初中后，慢慢习惯在县城住的生活，卫生条件也好一些。老家没有卫生间，电视没有有线，只有很少的几个台。我们也都开始青春期的叛逆，慢慢不愿回去那个看不了电视，上厕所环境不好，洗澡也没卫生间的老家。除了每年暑假例行的打红枣的时候，都回去帮忙，然后各小家分一包回去。枣树却越来越不给力，红枣数量逐年递减，还有隔壁的小孩也长大，学会爬树和偷枣，所以我们得回去看守枣树。直到初三那年，突然某天半夜还在打点滴却睡着的我被叫醒，点滴还没完就强行拔针，一家人匆忙得坐着救护车赶回老家，还是没能见上爷爷最后一面。爷爷早以写好自己的挽联，教过我们好几次，但是我们却都没太放在心上。到最后爷爷过世，没人能完整的记起挽联的内容，一直后悔不已。&lt;/p&gt;

&lt;p&gt;爷爷一直教育我们要好好读书。当年他考上师范的时候，全村人一起拱他读书。在这方面，他一直是全家人的榜样。他过世后，我们两兄弟也很争气，拿了我们初中的第二和第三名考上了高中。&lt;/p&gt;

&lt;p&gt;上了高中后，爸爸也买了摩托车，去老家就更方便了。摩托车比自行车快多了，而且在回老家的路上有一个又长又抖的坡，不再那么费力以至于得自己下车推着上坡了。村口也慢慢修了路，没有了那条弯曲而又满是泥泞的土路。基本上那会开始，在老家门口停的车，摩托比自行车要多了。大家庭的人口也在增长，表哥表姐陆续开始结婚，为家族添丁。还好老家门口的院子空间大，够摆放那么多车。&lt;/p&gt;

&lt;p&gt;上大学后，回老家的机会就更少了，基本不会在老家住了。交通也方便，再晚也能回到县城住。记得有一年暑假回去，弟弟说哥哥你们可回来了，就等你们来打红枣，树顶上那些都快被鸟儿吃完了。于是又干起小时候的活，忙活一阵把那三棵枣树上的红枣都打下来，但总共却只有很少很少了。洗好后大家分一分吃几颗，都快吃完了。枣树也年纪大了，红枣的产量早已不及当年的两大箩筐。&lt;/p&gt;

&lt;p&gt;老家也年纪大了，最近几年，只要一下大雨，爸爸就担心老家会不会倒。好像是大一那年，老家后的一间年代更久一点的房子在一个雷雨交加的夜晚倒塌了。在我的记忆中，小时候还在那间房子里玩过捉迷藏，我躲在里面的稻草堆里。因为是同样的土砖房，老家也难免会有同样的命运。不过由于老家曾经修缮过，把最底层的承重的土砖换成了红砖，使用年限又增加了几年。&lt;/p&gt;

&lt;p&gt;大家的交通工具又有了新变化，慢慢的，老家前的院子里不再只停着两个轮子的车子了。一两辆小车就要把院子停满。叔叔说以后要盖一间带大车库的新“老家”，老家的拆除也提上日程。该来的总要来的。。。&lt;/p&gt;

&lt;p&gt;现在的老家，只剩一些砖头了。为了不让老家在某个雨天轰然倒下，伤及还在老家住着的叔叔一家还有奶奶，也怕损坏老家的一些家具财务，所以还是选择了自己拆除。就在5月18号，开始拆除老家。在老家的原址上，将来会重新盖一栋新楼。也许会有一个大车库，就算以后兄弟们每人开辆车回去也能放下；也许会有一个精装修的卫生间，卫生条件也跟上城里的脚步；也许会有一个很大的院子，院子里还有三棵年老的枣树。。。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>关于我</title>
      <link>http://blog.ywheel.com/about/</link>
      <pubDate>Thu, 28 Apr 2016 23:52:12 +0800</pubDate>
      
      <guid>http://blog.ywheel.com/about/</guid>
      <description>

&lt;p&gt;Hi all, 我叫ywheel, 是一名程序员。&lt;/p&gt;

&lt;h3 id=&#34;我的经历&#34;&gt;我的经历&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;2006-2013 北京邮电大学本硕&lt;/li&gt;
&lt;li&gt;2013-2015 亚马逊（中国）SDE, Amazon Clicks中国团队最早成员&lt;/li&gt;
&lt;li&gt;2015.5.7-Now 某大数据公司平台研发主管&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;我的技能&#34;&gt;我的技能&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;主攻Java, 也能搞一些脚本语言如Python, perl, awk。正在学习scala&amp;hellip;&lt;/li&gt;
&lt;li&gt;目标全栈工程师，遗憾的是目前只能算略懂前端&lt;/li&gt;
&lt;li&gt;大数据技术栈, Hadoop, CDH, Oozie, Kafka, Storm, Spark&lt;/li&gt;
&lt;li&gt;逐步承担和和挑战架构设计&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;我的生活&#34;&gt;我的生活&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;2006-2015 北京九年，之后逃离&lt;/li&gt;
&lt;li&gt;爱打羽毛球&lt;/li&gt;
&lt;li&gt;琴棋书画，样样&amp;hellip;都会点，做一名文艺的程序员&lt;/li&gt;
&lt;li&gt;现实主义者，强迫症、焦虑症患者&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;一些链接&#34;&gt;一些链接&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Github: &lt;a href=&#34;https://github.com/ywheel&#34;&gt;github.com/ywheel&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;个人博客: &lt;a href=&#34;http://blog.ywheel.cn&#34;&gt;http://blog.ywheel.cn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;新浪微博: &lt;a href=&#34;http://weibo.com/ywheel&#34;&gt;weibo.com/ywheel&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Email: &lt;a href=&#34;mailto:ywheel@qq.com&#34;&gt;ywheel@qq.com&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;微信公众号：轮子们&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Grep命令的与或非</title>
      <link>http://blog.ywheel.com/post/2015/03/17/grep_and_or_not/</link>
      <pubDate>Tue, 17 Mar 2015 00:37:08 +0800</pubDate>
      
      <guid>http://blog.ywheel.com/post/2015/03/17/grep_and_or_not/</guid>
      <description>

&lt;pre&gt;&lt;code&gt;原文标题：
7 Linux Grep OR, Grep AND, Grep NOT Operator Examples
原文地址：
http://www.thegeekstuff.com/2011/10/grep-or-and-not-operators/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Question: Can you explain how to use OR, AND and NOT operators in Unix grep command with some examples?&lt;/p&gt;

&lt;p&gt;Answer: In grep, we have options equivalent to OR and NOT operators. There is no grep AND opearator. But, you can simulate AND using patterns. The examples mentioned below will help you to understand how to use OR, AND and NOT in Linux grep command.&lt;/p&gt;

&lt;p&gt;The following employee.txt file is used in the following examples.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cat employee.txt
100  Thomas  Manager    Sales       $5,000
200  Jason   Developer  Technology  $5,500
300  Raj     Sysadmin   Technology  $7,000
400  Nisha   Manager    Marketing   $9,500
500  Randy   Manager    Sales       $6,000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You already knew that grep is extremely powerful based on these &lt;a href=&#34;http://www.thegeekstuff.com/2009/03/15-practical-unix-grep-command-examples/&#34;&gt;grep command examples&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;grep-or-operator&#34;&gt;Grep OR Operator&lt;/h3&gt;

&lt;p&gt;Use any one of the following 4 methods for grep OR. I prefer method number 3 mentioned below for grep OR operator.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Grep OR Using |&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;If you use the grep command without any option, you need to use | to separate multiple patterns for the or condition.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;grep &#39;pattern1\|pattern2&#39; filename
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For example, grep either Tech or Sales from the employee.txt file. Without the back slash in front of the pipe, the following will not work.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ grep &#39;Tech\|Sales&#39; employee.txt
100  Thomas  Manager    Sales       $5,000
200  Jason   Developer  Technology  $5,500
300  Raj     Sysadmin   Technology  $7,000
500  Randy   Manager    Sales       $6,000
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;Grep OR Using -E&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;grep -E option is for extended regexp. If you use the grep command with -E option, you just need to use | to separate multiple patterns for the or condition.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;grep -E &#39;pattern1|pattern2&#39; filename
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For example, grep either Tech or Sales from the employee.txt file. Just use the | to separate multiple OR patterns.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ grep -E &#39;Tech|Sales&#39; employee.txt
100  Thomas  Manager    Sales       $5,000
200  Jason   Developer  Technology  $5,500
300  Raj     Sysadmin   Technology  $7,000
500  Randy   Manager    Sales       $6,000
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;Grep OR Using egrep&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;egrep is exactly same as ‘grep -E’. So, use egrep (without any option) and separate multiple patterns for the or condition.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;egrep &#39;pattern1|pattern2&#39; filename
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For example, grep either Tech or Sales from the employee.txt file. Just use the | to separate multiple OR patterns.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ egrep &#39;Tech|Sales&#39; employee.txt
100  Thomas  Manager    Sales       $5,000
200  Jason   Developer  Technology  $5,500
300  Raj     Sysadmin   Technology  $7,000
500  Randy   Manager    Sales       $6,000
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;Grep OR Using grep -e&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Using grep -e option you can pass only one parameter. Use multiple -e option in a single command to use multiple patterns for the or condition.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;grep -e pattern1 -e pattern2 filename
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For example, grep either Tech or Sales from the employee.txt file. Use multiple -e option with grep for the multiple OR patterns.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ grep -e Tech -e Sales employee.txt
100  Thomas  Manager    Sales       $5,000
200  Jason   Developer  Technology  $5,500
300  Raj     Sysadmin   Technology  $7,000
500  Randy   Manager    Sales       $6,000
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;grep-and&#34;&gt;Grep AND&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;Grep AND using -E ‘pattern1.*pattern2′&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;There is no AND operator in grep. But, you can simulate AND using grep -E option.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;grep -E &#39;pattern1.*pattern2&#39; filename
grep -E &#39;pattern1.*pattern2|pattern2.*pattern1&#39; filename
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The following example will grep all the lines that contain both “Dev” and “Tech” in it (in the same order).&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ grep -E &#39;Dev.*Tech&#39; employee.txt
200  Jason   Developer  Technology  $5,500
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The following example will grep all the lines that contain both “Manager” and “Sales” in it (in any order).&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ grep -E &#39;Manager.*Sales|Sales.*Manager&#39; employee.txt
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note: Using &lt;a href=&#34;http://www.thegeekstuff.com/2011/01/advanced-regular-expressions-in-grep-command-with-10-examples-%E2%80%93-part-ii/&#34;&gt;regular expressions in grep&lt;/a&gt; is very powerful if you know how to use it effectively.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Grep AND using Multiple grep command&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;You can also use multiple grep command separated by pipe to simulate AND scenario.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;grep -E &#39;pattern1&#39; filename | grep -E &#39;pattern2&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The following example will grep all the lines that contain both “Manager” and “Sales” in the same line.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ grep Manager employee.txt | grep Sales
100  Thomas  Manager    Sales       $5,000
500  Randy   Manager    Sales       $6,000
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;grep-not&#34;&gt;Grep NOT&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;Grep NOT using grep -v&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Using grep -v you can simulate the NOT conditions. -v option is for invert match. i.e It matches all the lines except the given pattern.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;grep -v &#39;pattern1&#39; filename
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For example, display all the lines except those that contains the keyword “Sales”.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ grep -v Sales employee.txt
200  Jason   Developer  Technology  $5,500
300  Raj     Sysadmin   Technology  $7,000
400  Nisha   Manager    Marketing   $9,500
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can also combine NOT with other operator to get some powerful combinations.&lt;/p&gt;

&lt;p&gt;For example, the following will display either Manager or Developer (bot ignore Sales).&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ egrep &#39;Manager|Developer&#39; employee.txt | grep -v Sales
200  Jason   Developer  Technology  $5,500
400  Nisha   Manager    Marketing   $9,500
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;总结&#34;&gt;总结&lt;/h3&gt;

&lt;p&gt;总结起来就是：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;OR: \|, -E， -e, egrep
AND: -E, grep | grep
NOT: -v
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>LeetCode 119 Pascal&#39;s Triangle II</title>
      <link>http://blog.ywheel.com/post/2015/03/17/leetcode_119/</link>
      <pubDate>Tue, 17 Mar 2015 00:16:08 +0800</pubDate>
      
      <guid>http://blog.ywheel.com/post/2015/03/17/leetcode_119/</guid>
      <description>&lt;p&gt;题：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;https://leetcode.com/problems/pascals-triangle-ii/
Given an index k, return the kth row of the Pascal&#39;s triangle.

For example, given k = 3,
Return [1,3,3,1].

Note:
Could you optimize your algorithm to use only O(k) extra space?
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;解法较容易，时间复杂度依然是0(n^2)，要实现空间复杂度得考虑在内层循环时从后往前看，这样的话上一行的结果不会被覆盖。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;public class Solution {  
    public List&amp;lt;Integer&amp;gt; getRow(int rowIndex) {  
        if (rowIndex &amp;lt; 0) return null;  
        List&amp;lt;Integer&amp;gt; results = new ArrayList&amp;lt;Integer&amp;gt;(rowIndex + 1);  
        for (int row=0; row &amp;lt; rowIndex; row++) {  
            results.add(1);  
            for (int i=row; i&amp;gt;0; i--) {  
                results.set(i, results.get(i - 1) + results.get(i));  
            }  
        }  
        results.add(1); // add last 1  
        return results;  
    }  
}  
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>LeetCode 023 Merge K Sorted Lists</title>
      <link>http://blog.ywheel.com/post/2015/03/12/leetcode_23/</link>
      <pubDate>Thu, 12 Mar 2015 02:52:08 +0800</pubDate>
      
      <guid>http://blog.ywheel.com/post/2015/03/12/leetcode_23/</guid>
      <description>

&lt;p&gt;题：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;https://leetcode.com/problems/merge-k-sorted-lists/
Merge k sorted linked lists and return it as one sorted list. Analyze and describe its complexity.
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;解法一&#34;&gt;解法一&lt;/h3&gt;

&lt;p&gt;将K个链表做K-1次归并，每次归并是对两个链表的归并，最终得到一个排序的链表。即：
1，2合并，遍历2n个节点；(1,2)与3合并，遍历3n个节点。。。(1&amp;hellip;k-1)与k合并，遍历kn个节点。所以总共遍历n&lt;em&gt;(2+3+&amp;hellip;+k)=n&lt;/em&gt;(k^2+k-2)/2，那么时间复杂度为O(n*k^2)。&lt;/p&gt;

&lt;h3 id=&#34;解法二&#34;&gt;解法二&lt;/h3&gt;

&lt;p&gt;对解法一改进一下，改用分治法，所以时间复杂度变为O(nklogk)。&lt;/p&gt;

&lt;h3 id=&#34;解法三&#34;&gt;解法三&lt;/h3&gt;

&lt;p&gt;将K个链表的首元素都取出来，选择出最小的那个作为新链表的head。然后将该元素的next取出来，与其他链表的元素比较再选一个小的，放到新链表中。选择出最小元素的时间复杂度为O(k), 总共要选nk次，所以时间复杂度为O(n*k^2)。&lt;/p&gt;

&lt;h3 id=&#34;解法四&#34;&gt;解法四&lt;/h3&gt;

&lt;p&gt;对解法三改进一下，用最小堆来实现选择最小元素的要求，则时间复杂度降为O(logk)，则总的时间复杂度为O(nklogk)，与解法二的时间复杂度一样。在Java中，可以使用基于
最小堆算法的PriorityQueue来作为最小堆的集合类。代码如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;/** 
 * Definition for singly-linked list. 
 * public class ListNode { 
 *     int val; 
 *     ListNode next; 
 *     ListNode(int x) { 
 *         val = x; 
 *         next = null; 
 *     } 
 * } 
 */  
public class Solution {  
    public ListNode mergeKLists(List&amp;lt;ListNode&amp;gt; lists) {  
        if (lists == null || lists.isEmpty()) {  
            return null;  
        }  
        PriorityQueue&amp;lt;ListNode&amp;gt; minHeap = new PriorityQueue&amp;lt;ListNode&amp;gt;(new Comparator&amp;lt;ListNode&amp;gt;(){  
            public int compare(ListNode l1, ListNode l2) {  
                return Integer.compare(l1.val, l2.val);  
            }  
        });  
        for (ListNode node : lists) {  
            if (node != null) {  
                minHeap.add(node);  
            }  
        }  
        ListNode helper = new ListNode(0);  
        ListNode next = helper;  
        while (!minHeap.isEmpty()) {  
            ListNode min = minHeap.poll(); // must not be null  
            next.next = min;  
            min = min.next;  
            if (min != null) {  
                minHeap.add(min);  
            }  
            next = next.next;  
        }  
        return helper.next;  
    }  
}  
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>LeetCode 021 Merge Two Sorted Lists</title>
      <link>http://blog.ywheel.com/post/2015/03/12/leetcode_21/</link>
      <pubDate>Thu, 12 Mar 2015 01:43:08 +0800</pubDate>
      
      <guid>http://blog.ywheel.com/post/2015/03/12/leetcode_21/</guid>
      <description>&lt;p&gt;题：&lt;/p&gt;

&lt;p&gt;````
&lt;a href=&#34;https://leetcode.com/problems/merge-two-sorted-lists/&#34;&gt;https://leetcode.com/problems/merge-two-sorted-lists/&lt;/a&gt;
Merge two sorted linked lists and return it as a new list. The new list should be made by splicing together the nodes of the first two lists.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
解法一：创建一个新的链表：

```java
/** 
 * Definition for singly-linked list. 
 * public class ListNode { 
 *     int val; 
 *     ListNode next; 
 *     ListNode(int x) { 
 *         val = x; 
 *         next = null; 
 *     } 
 * } 
 */  
public class Solution {  
    public ListNode mergeTwoLists(ListNode l1, ListNode l2) {  
        if (l1 == null &amp;amp;&amp;amp; l2 == null) {  
            return null;  
        } else if (l1 == null) {  
            return l2;  
        } else if (l2 == null) {  
            return l1;  
        } else {  
            ListNode head = null;  
            ListNode next = new ListNode(0); // before head  
            while (l1 != null || l2 != null) {  
                if (l1 != null &amp;amp;&amp;amp; l2 != null) {  
                    if (l1.val &amp;lt; l2.val) {  
                        next.next = l1;  
                        l1 = l1.next;  
                    } else {  
                        next.next = l2;  
                        l2 = l2.next;  
                    }  
                } else if (l1 == null) {  
                    next.next = l2;  
                    l2 = l2.next;  
                } else {  
                    next.next = l1;  
                    l1 = l1.next;  
                }  
                head = head == null ? next.next : head;  
                next = next.next;  
            }  
            return head;  
        }  
    }  
}  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;解法二：将l2加入到l1中&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;/** 
 * Definition for singly-linked list. 
 * public class ListNode { 
 *     int val; 
 *     ListNode next; 
 *     ListNode(int x) { 
 *         val = x; 
 *         next = null; 
 *     } 
 * } 
 */  
public class Solution {  
    public ListNode mergeTwoLists(ListNode l1, ListNode l2) {  
        if (l1 == null &amp;amp;&amp;amp; l2 == null) {  
            return null;  
        } else if (l1 == null) {  
            return l2;  
        } else if (l2 == null) {  
            return l1;  
        } else {  
            // merge l2 to l1  
            ListNode head = null;  
            ListNode prev = null;  
            while (l2 != null) {  
                if (l1 == null) {  
                    prev.next = l2;  
                    break;  
                }  
                if (l1.val &amp;gt; l2.val) {  
                    if (prev == null) {  
                        prev = l2;  
                    } else {  
                        prev.next = l2;  
                    }  
                    ListNode node = l2.next;  
                    l2.next = l1;  
                    prev = l2;  
                    l2 = node;  
                } else {  
                    prev = l1;  
                    l1 = l1.next;  
                }  
                head = head == null ? prev : head;  
                  
            }  
            return head;  
        }  
    }  
}  
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>LeetCode 088 Merge Sorted Array</title>
      <link>http://blog.ywheel.com/post/2015/03/12/leetcode_88/</link>
      <pubDate>Thu, 12 Mar 2015 00:36:08 +0800</pubDate>
      
      <guid>http://blog.ywheel.com/post/2015/03/12/leetcode_88/</guid>
      <description>&lt;p&gt;题：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;https://leetcode.com/problems/merge-sorted-array/
Given two sorted integer arrays A and B, merge B into A as one sorted array.

Note:
You may assume that A has enough space (size that is greater or equal to m + n) to hold additional elements from B. The number of elements initialized in A and B are m and n respectively.
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;public class Solution {  
    public void merge(int A[], int m, int B[], int n) {  
        if (A == null || B == null || A.length == 0 || B.length == 0 || A.length &amp;lt;  m+n) {  
            return;  
        }  
        int a = m - 1;  
        int b = n - 1;  
        for (int i=m+n-1; i&amp;gt;=0; i--) {  
            if (a &amp;gt;=0 &amp;amp;&amp;amp; b &amp;gt;= 0) {  
                if (A[a] &amp;gt; B[b]) {  
                    A[i] = A[a--];  
                } else {  
                    A[i] = B[b--];  
                }  
            } else if (a &amp;lt; 0) {  
                // only left B  
                System.arraycopy(B, 0, A, 0, i+1);  
            } // else {  
                // only left A, do nothing  
            //}  
        }  
    }  
}  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;往事不堪回首，当初某游戏公司的面试尽然倒在这题上，哎。。。说多了都是泪&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>